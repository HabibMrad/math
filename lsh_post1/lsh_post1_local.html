<!doctype html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="format-detection" content="telephone=no">
    <title>Introduction to Locality-Sensitive Hashes</title>
	<script type="text/x-mathjax-config"> 
      MathJax.Hub.Register.StartupHook("TeX AMSmath Ready",function () { 
        MathJax.InputJax.TeX.Definitions.environment["densearray"] = 
          ['AMSarray',null,true,true,'rcl','0em .4em']; 
      }); 
	</script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
    <link rel="stylesheet" href="tufte-edited.css"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
  </head>
  <body>
    <p style="display:none">\(\newcommand{\latexonlyrule}[2]{}\)</p>
    <div class="main">
      <div class="title">Introduction to Locality-Sensitive Hashes</div>
      <div class="author">Tyler Neylon</div>
      <div class="date">145.2018</div>
      <p>CHECK:</p>
      <ul>
      <li>Search this file for all instances of CHECK, IMAGE, XXX</li>
      <li>Check that the two pdf versions look good, including cross-references</li>
      <li>Check that the references header has no number</li>
      <li>Try to nicify the images in the pdf files</li>
      <li>Ensure that all references to figures in the text are done by mentioning a figure number.</li>
      </ul>
      <p><em>Locality-sensitive hashes</em> are techniques that dramatically speed up search-for-neighbors or near-duplication detection on data. They can be used, for example, to filter out duplicates of scraped web pages at an impressive speed, or to perform near-constant-time lookups of nearby points from a geospatial data set.</p>
      <p>When you think about hash functions, you might think about <em>hash tables</em>, which is perhaps the most common use case. As a reminder, the hash functions used in a hash table are designed to map a data structure to an integer that can be used to look in a particular <em>bucket</em> within the hash table to retrieve (or delete) that element. Common containers with string keys like JavaScript object attributes and Python dictionaries are based on hash tables. Although they might not <em>guarantee</em> constant-time lookups, in practice they effectively provide them. Hash functions used for hash tables are called <em>universal hash functions</em>. [CHECK]</p>
      <p>There are a number of other classes of hash functions as well. For example the SHA1 cryptographic hash function is designed to be <em>difficult to reverse</em>, which is useful if you want to store someone’s password as a hashed value. [CHECK] Another security-oriented hash function is CHECK, which is actually designed to be <em>expensive to compute</em>, as this can deter malicious ne’er-do-wells from easily building large lookup tables to be able to reverse a hash on more likely input values. Hash functions like these are called <em>secure hash functions</em>. [CHECK]</p>
      <p>Here are what all these various hash functions have in common:</p>
      <ul>
      <li>They map a wide variety of input data types to discrete values.</li>
      <li>In practice, we care about whether or not two (or more) input values map to the same output (hashed) value.</li>
      </ul>
      <p>Locality-sensitive hash (LSH) functions are specifically designed so that collisions of the hash value are <em>more likely</em> given two input values that are <em>close together</em>. Just as there are different implementations of secure hash functions for different use cases, there are different implementations of LSH functions for different data types and for different definitions of being <em>close together</em>. In this post, I’ll give a brief overview of the key ideas, and take a look at a toy example based on <em>random projections</em> of vectors into lower-dimensional spaces.</p>
      <h1 id="an-example"><span class="header-section-number">1</span> An example</h1>
      <p>It will probably be much easier to grasp the main idea with an example. (The “toy example” for random projections will come later. This is like a mini-toy example.)</p>
      <p>Suppose you have a million people from across the United States all standing in a huge room. It’s your job to get people who live close together to stand together in groups. Imagine how much time it would take to walk up to each person, ask for their street address, map that to a lat/long pair, then write some code to find reasonable geographic clusters, and walk up to every person again and tell them their cluster number. It’s a disaster.</p>
      <p>Here’s a much better way to solve this problem: Write every U.S. zip code on poster boards, and hang those from the ceiling. Then announce to everyone to go stand under the zip code where they live.</p>
      <p>Voila! That’s much easier, right? The main idea here is also the main idea behind locality-sensitive hashes. We’re taking an arbitrary data type (a person, who we could of as a ton of data including their street address), and mapping that data into a set of discrete values (zip codes) such that people who live close together probably hash to the same value. In other words, the clusters are very likely to be groups of neighbors.</p>
      <p>The distinction between walking sequentially up to each person versus parallelizing the work by asking everyone to find their own way to their zip code was not an accident. Besides avoiding whatever clustering algorithm you’d have to run on lat/long coordinates, another advantage of this hashing approach is that it’s extremely friendly to parallel processing. Despite caring about <em>relationships</em> within your data, you can still split up the data any way you like and compute the hashes in a fully parallelized fashion.</p>
      <p>Another property of this example is that it is <em>approximate</em> in the sense that some people may live across the street from each other, but happen to cross a zip code line, in which case they would not be clustered together here. As we’ll see below, it’s also possible for data points to be clustered together even when they’re very far apart, although a well-designed LSH can at least give some mathematical evidence that this will be a rare event, and some implementations manage to guarantee certain bad cases (such as clustering of very far points or non-clustering of very close points) never happen.</p>
      <h1 id="hashing-points-via-projection"><span class="header-section-number">2</span> Hashing points via projection</h1>
      <p>Let’s start with an incredibly simple mathematical function that we can treat as an LSH. Define <span class="math inline">\(f:{\mathbb{R}}^2 \to {\mathbb{Z}}\)</span> for a point <span class="math inline">\(x\in{\mathbb{R}}^2\)</span> by</p>
      <p><span class="math display">\[ f(x) := \lfloor x_1 \rfloor; \]</span></p>
      <p>that is <span class="math inline">\(f(x)\)</span> is the largest integer <span class="math inline">\(a\)</span> for which <span class="math inline">\(a\le x_1.\)</span> (For example, <span class="math inline">\(f((3.2, -1.2)) = 3.\)</span>)</p>
      <p>Let’s suppose we choose points at random by uniformly sampling from the origin-centered circle <span class="math inline">\(\mathcal C\)</span> with radius 3:</p>
      <p><span class="math display">\[ \mathcal C := \{ (x, y) : x^2 + y^2 \le 3^2 \}. \]</span></p>
      <p>If we want to find which of our points in <span class="math inline">\(\mathcal C\)</span> are close together, we can estimate this relationship by clustering together points <span class="math inline">\(a\)</span> and <span class="math inline">\(b \in \mathcal C\)</span> iff (if and only if) <span class="math inline">\(f(a) = f(b).\)</span> It will be handy to introduce the notation <span class="math inline">\(a \sim b\)</span> to indicate that <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are in the same cluster. With that notation, we can write our current hash setup as</p>
      <p><span class="math display">\[ a \sim b \iff h_1(a) = h_1(b). \]</span></p>
      <p>Figure 1 shows an example of such a clustering.</p>
      <div class="figure">
      <p class="caption">Figure 1: Twenty points chosen randomly in a circle with radius 4. Each point <span class="math inline">\(x\)</span> is colored based on its hash value <span class="math inline">\(h_1(x).\)</span></p>
      <img src="images/lsh_image1_v2.png" alt="Figure 1: Twenty points chosen randomly in a circle with radius 4. Each point x is colored based on its hash value h_1(x)." id="fig:fig1" />
      </div>
      <p>You can immediately see that some points are far apart yet clustered, while others are relatively close yet unclustered. There’s also a sense that this particular hash function <span class="math inline">\(h_1\)</span> was arbitrarily chosen to focus on the x-axis. What would have happened with the same data if we had used instead <span class="math inline">\(h_2(x) := \lfloor x_2 \rfloor?\)</span> The result is figure 2.</p>
      <div class="figure">
      <p class="caption">Figure 2: The same twenty points as figure 1, except that we’re using the <span class="math inline">\(y\)</span> values (instead of <span class="math inline">\(x\)</span> values) to determine the hash-based cluster colors this time around.</p>
      <img src="images/lsh_image2.png" alt="Figure 2: The same twenty points as figure 1, except that we’re using the y values (instead of x values) to determine the hash-based cluster colors this time around." id="fig:fig2" />
      </div>
      <p>While neither clustering alone is amazing, things start to work better if we use both of them simultaneously. That is, we can redefine our clustering via</p>
      <p><span id="eq:eq1"><span class="math display">\[ a \sim b \iff h_1(a) = h_1(b) \text{ and } h_2(a) = h_2(b). \qquad(1)\]</span></span></p>
      <p>Our same example points are shown under this new clustering in figure 3.</p>
      <div class="figure">
      <p class="caption">Figure 3: The same twenty points clustered via two different hashes — one using <span class="math inline">\(\lfloor x\rfloor\)</span>, the other using <span class="math inline">\(\lfloor y\rfloor.\)</span> As before, points are colored based on which cluster they’re in; a cluster is the set of all points who share both their <span class="math inline">\(\lfloor x\rfloor\)</span> and their <span class="math inline">\(\lfloor y\rfloor\)</span> values.</p>
      <img src="images/lsh_image3.png" alt="Figure 3: The same twenty points clustered via two different hashes — one using \lfloor x\rfloor, the other using \lfloor y\rfloor. As before, points are colored based on which cluster they’re in; a cluster is the set of all points who share both their \lfloor x\rfloor and their \lfloor y\rfloor values." id="fig:fig3" />
      </div>
      <h2 id="randomizing-our-hashes"><span class="header-section-number">2.1</span> Randomizing our hashes</h2>
      <p>So far we’ve defined deterministic hash functions. Let’s change that by choosing a random rotation matrix <span class="math inline">\(U\)</span> (a rotation around the origin) along with a random offset <span class="math inline">\(b \in [0, 1).\)</span> Given such a random <span class="math inline">\(U\)</span> and <span class="math inline">\(b,\)</span> we could define a new hash function via</p>
      <p><span class="math display">\[ h(x) := \lfloor (Ux)_1 + b \rfloor, \]</span></p>
      <p>where I’m using the notation <span class="math inline">\(( \textit{vec} )_1\)</span> to indicate the first coordinate of the vector value <em>vec</em> (that is, the notation <span class="math inline">\((Ux)_1\)</span> means the first coordinate of the vector <span class="math inline">\(Ux\)</span>).</p>
      <p>It may seem a tad arbitrary to use only the first coordinate here rather than any other, but the fact that we’re taking a random rotation first means that we have the same set of possibilities, with the same probability distribution, as we would when pulling out any other single coordinate value.</p>
      <p>The advantage of using randomized hash functions is that any theoretical properties we want to discuss will apply without having to worry about pathologically weird data. Conceptually, if we were using deterministic hash functions, then someone could choose the worst-case data for our hash function, and we’d be stuck with that poor performance (for example, choosing maximally-far apart points that are still clustered together by our <span class="math inline">\(h_1\)</span> function above). By using randomly chosen hash functions, we can ensure that any average-case behavior of our hash functions applies equally well to <em>all data</em>. This same perspective is useful for hash tables in the form of <em>universal hashing</em>; if randomized hash functions are a new idea for you, I recommend checking out <a href="https://en.wikipedia.org/wiki/Universal_hashing">Wikipedia’s universal hashing page</a>.</p>
      <p>Let’s revisit the example points we used above, but now apply some randomized hash functions. In figure CHECK, points are clustered iff both of their hash values (from <span class="math inline">\(h_1()\)</span> and <span class="math inline">\(h_2()\)</span>) collide. We’ll use that same idea, but this time choose four rotations <span class="math inline">\(U_1, \ldots, U_4\)</span> as well as four offsets <span class="math inline">\(b_1, \ldots, b_4\)</span> to define <span class="math inline">\(h_1(), \ldots, h_4()\)</span> via</p>
      <p><span class="math display">\[ h_i(x) := \lfloor (U_i x)_1 + b_i \rfloor. \]</span></p>
      <p>Here’s the resulting clustering:</p>
      <p>IMAGE 4</p>
      <p>It’s not obvious that we actually want to use all four of our hash functions. The issue is that our clusters have become quite small. There are a couple ways to address this. One is to simply increase the scale of the hash functions; for example:</p>
      <p><span class="math display">\[ \tilde h_i(x) := h_i(x/s), \]</span></p>
      <p>where <span class="math inline">\(s\)</span> is a scale factor (larger <span class="math inline">\(s\)</span> values will result in larger clusters).</p>
      <p>However, there is something a bit more nuanced we can look at, which is to allow some adaptability in terms of <em>how many hash collisions we require</em>. In other words, suppose we have <span class="math inline">\(k\)</span> total hash functions (just above, we had <span class="math inline">\(k=4\)</span>). Instead of insisting that all <span class="math inline">\(k\)</span> hash values must match before we say two points are in the same cluster, we could look at cases where some number <span class="math inline">\(j \le k\)</span> of them matches. To state this mathematically, we would rewrite equation (1) as</p>
      <p><span id="eq:eq2"><span class="math display">\[ a \sim b \iff \#\{i: h_i(a) = h_i(b)\} \ge j. \qquad(2)\]</span></span></p>
      <p>Something interesting happens here, which is that the <span class="math inline">\(a \sim b\)</span> relationship is no longer a clustering, but becomes more like adjacency (that is, sharing an edge) in a graph. The difference is that, in a clustering, if <span class="math inline">\(a\sim b\)</span> and <span class="math inline">\(b\sim c,\)</span> the we must have <span class="math inline">\(a\sim c\)</span> as well; this is called being <em>transitively closed</em>. Graphs don’t need to have this property, and in our case as well, it’s no longer true that our similarity relationship is transitively closed.</p>
      <p>It may help your intuition to visualize this new definition of <span class="math inline">\(a\sim b\)</span> on our previous data points:</p>
      <p>IMAGE 5</p>
      <p>In fact, we can visualize all possible cutoff values (values of <span class="math inline">\(j\)</span> in (2)) in a single image by using weighted edges, like so:</p>
      <p>IMAGE 6</p>
      <p>Yet another fun way to get an intuitive feel for how much information we’re getting from our hashes is to see which subsets of our circle are matched — and to what degree — by a given point:</p>
      <p>IMAGE 8</p>
      <h2 id="why-this-is-faster"><span class="header-section-number">2.2</span> Why this is faster</h2>
      <p>So far we’ve been sticking to 2-dimensional data because that’s easier to visualize in an article. However, if you think about computing 10 hashes for every 2-dimensional point in order to find neighbors, it may feel like you’re doing more work than the simple solution of a linear search through your points. Let’s review cases where using an LSH is more efficient than other methods of finding nearby points.</p>
      <h3 id="zero-linear-search"><span class="header-section-number">2.2.1</span> Zero linear search</h3>
      <p>If you have a huge number <span class="math inline">\(n\)</span> of points, and it’s reasonable for you to index those points ahead of time — meaning, you can afford to compute all <span class="math inline">\(k\)</span> hash values for each point — then you can completely avoid the linear-time cost of a brute force search for nearby points given a new query point. This speed-up is relevant in any dimension, including the simple 2-dimensional case.</p>
      <h3 id="fewer-hashes-needed-in-higher-dimensions"><span class="header-section-number">2.2.2</span> Fewer hashes needed in higher dimensions</h3>
      <p>Another effect that may be less obvious is that you can get away with fewer hash values (a smaller <span class="math inline">\(k\)</span> value) in higher dimensions. There are some mathematically sophisticated ways to quantify that statement, but it may be even easier to understand graph based on empirically derived data.</p>
      <p>Here’s a summary of some random sampling I did in order to explore the relationship between various values of <span class="math inline">\(j\)</span> for <span class="math inline">\(d=100\)</span> dimensional data using <span class="math inline">\(k=10\)</span> different random hashes: CHECK</p>
      <p>IMAGE 9</p>
      <p>CHECK the whole next paragraph What’s interesting here is that we get a relatively tight box plot for <span class="math inline">\(j\)</span> values around CHECK. This means that we can choose the threshold <span class="math inline">\(j=CHECK\)</span> in equation (2) and have fairly good confidence that our hash-based “nearby” relationship closely matches reality.</p>
      <p>We can even quantify this precisely. Although this article doesn’t <em>prove</em> the following implications, the empirical evidence found CHECK(add link to code) strongly suggests that these are in fact the correct values:</p>
      <p><span class="math display">\[ \text{dist}(a, b) &gt; \alpha \Rightarrow P(\#\{i : h_i(a) = h_i(b)\} &lt; j) &gt; 0.95; \]</span></p>
      <p><span class="math display">\[ \text{dist}(a, b) &lt; \beta  \Rightarrow P(\#\{i : h_i(a) = h_i(b)\} \ge j) &lt; 0.05. \]</span></p>
      <p>CHECK(the actual identities may end up being based on the left side using a <span class="math inline">\(j\)</span> value rather than a distance to start with).</p>
      <p>We might interpret these last two expressions as saying that we believe at least 99% of our pairwise relationships are correctly classified. And we’re able to do so while saving about <span class="math inline">\(O(n)\)</span> speed.</p>
      <h2 id="other-data-types-and-approaches"><span class="header-section-number">2.3</span> Other data types and approaches</h2>
      <p>This article has focused on numeric, 2-dimensional data because it’s easier to visualize. Locality-sensitive hashes can certainly be used for many other data types, including strings, sets, or high-dimensional vectors.</p>
      <p>There are also other ways to specifically measure the performance of a particular hashing approach. For example, CHECK.</p>
      <p>Yet another ingredient to throw into the mix here are techniques to boost performance which can treat any LSH as a black box. My favorite approach here is to simply perform multiple lookups on a hash system, each time using <span class="math inline">\(q + \varepsilon\)</span> as an input, where <span class="math inline">\(q\)</span> is your query value, and <span class="math inline">\(\varepsilon\)</span> is a random variable centered at zero. CHECK(add a bit about what this achieves; add a reference for it)</p>
      <p>There’s a lot more that can be said about LSH techniques. If there is reader interest, I may write a follow-up article explaining the details of min-wise hashing, which is a fun case that’s simultaneously good at quickly finding nearby sets as well as nearby strings.</p>
      <p>CHECK ensure that the references section is not numbered</p>
      <hr />
      <h1 id="references"><span class="header-section-number">3</span> References</h1>
    </div>
  </body>
</html>

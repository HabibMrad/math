<!doctype html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="format-detection" content="telephone=no">
    <title>Introduction to Locality-Sensitive Hashes</title>
	<script type="text/x-mathjax-config"> 
      MathJax.Hub.Register.StartupHook("TeX AMSmath Ready",function () { 
        MathJax.InputJax.TeX.Definitions.environment["densearray"] = 
          ['AMSarray',null,true,true,'rcl','0em .4em']; 
      }); 
	</script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
    <link rel="stylesheet" href="tufte-edited.css"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
  </head>
  <body>
    <p style="display:none">\(\newcommand{\latexonlyrule}[2]{}\)</p>
    <div class="main">
      <div class="title">Introduction to Locality-Sensitive Hashes</div>
      <div class="author">Tyler Neylon</div>
      <div class="date">145.2018</div>
      <p>CHECK:</p>
      <ul>
      <li>Update images to all use unitRadius = 2 (or consider it)</li>
      <li>I think figure 4 only has 3 hashes and the text says 4 Try updating it to really have 4 hashes, but I can fall back to 3 if 4 looks bad.</li>
      <li>Search this file for all instances of CHECK, IMAGE, XXX</li>
      <li>Check that the two pdf versions look good, including cross-references</li>
      <li>Check that the references header has no number</li>
      <li>Try to nicify the images in the pdf files</li>
      <li>Ensure that all references to figures in the text are done by mentioning a figure number.</li>
      <li>Follow-up ideas (put these somewhere centralized with other Unbox post ideas)</li>
      <li>MinHash</li>
      <li>ProxHash</li>
      <li>The J-L Lemma</li>
      </ul>
      <p><em>Locality-sensitive hashing</em> (LSH) is a set of techniques that dramatically speed up search-for-neighbors or near-duplication detection on data. These techniques can be used, for example, to filter out duplicates of scraped web pages at an impressive speed, or to perform near-constant-time lookups of nearby points from a geospatial data set.</p>
      <p>Outside of the LSH world, a common use for hash functions is in <em>hash tables</em>. As a reminder, the hash functions used in a hash table are designed to map a piece of data to an integer that can be used to look in a particular <em>bucket</em> within the hash table to retrieve or delete that element. Many containers with string keys, such as JavaScript objects or Python dictionaries, are based on hash tables. Although hash tables might not <em>guarantee</em> constant-time lookups, in practice they effectively provide them. If you’re curious, <a href="https://en.wikipedia.org/wiki/Universal_hashing"><em>universal hashing</em></a> is a hashing approach that’s useful in hash table implementations.</p>
      <p>There are other classes of hash functions as well. For example, the <a href="https://en.wikipedia.org/wiki/SHA-1">SHA-1</a> cryptographic hash function is designed to be <em>difficult to reverse</em>, which is useful if you want to store someone’s password as a hashed value. Hash functions like these are called <a href="https://en.wikipedia.org/wiki/Cryptographic_hash_function"><em>cryptographic hash functions</em></a>.</p>
      <p>All hash functions have these key properties:</p>
      <ul>
      <li>They map some fixed type of input (such as strings or floats) to discrete values.</li>
      <li>They’re designed so that different input values map to the same output (hashed) value in a manner useful to the application at hand.</li>
      </ul>
      <p>Locality-sensitive hash functions are specifically designed so that hash value collisions are <em>more likely</em> for two input values that are <em>close together</em>. Just as there are different implementations of secure hash functions for different use cases, there are different implementations of LSH functions for different data types and for different definitions of being <em>close together</em>. In this post, I’ll give a brief overview of the key ideas, and take a look at a simple example based on <em>random projections</em> (defined below) of vectors into lower-dimensional spaces.</p>
      <h1 id="a-human-example"><span class="header-section-number">1</span> A human example</h1>
      <p>It will probably be much easier to grasp the main idea with an example you can relate to. (The random projection example will be next.)</p>
      <p>Suppose you have a million people from across the United States all standing in a huge room. It’s your job to get people who live close together to stand in their own groups. Imagine how much time it would take to walk up to each person, ask for their street address, map that to a lat/long pair, then write code to find geographic clusters, and walk up to every person again and tell them their cluster number. It’s a disaster.</p>
      <p>Here’s a much better way to solve this problem: Write every U.S. zip code on poster boards and hang those from the ceiling. Then tell everyone to go stand under the zip code where they live.</p>
      <p>Voila! That’s much easier, right? The main idea here is also the main idea behind locality-sensitive hashes. We’re taking an arbitrary data type (a person, who we can think of as a ton of data including their street address), and mapping that data into a set of discrete values (zip codes) such that people who live close together probably hash to the same value. In other words, the clusters (people with the same zip code) are very likely to be groups of neighbors.</p>
      <p>A nice benefit of the zip code appraoch is that it’s parallel-friendly. Instead of requiring a center of communication, every person can walk directly to their destination without further coordination. This is a bit surprising in light of the fact that the result (clusters of neighbors) is based entirely on the <em>relationships</em> between the inputs.</p>
      <p>Another property of this example is that it is <em>approximate</em>: some people may live across the street from each other, but happen to have different zip codes, in which case they would not be clustered together here. As we’ll see below, it’s also possible for data points to be clustered together even when they’re very far apart, although a well-designed LSH can at least give some mathematical evidence that this will be a rare event, and some implementations manage to guarantee this can never happen.</p>
      <h1 id="hashing-points-with-projections"><span class="header-section-number">2</span> Hashing points with projections</h1>
      <p>Let’s start with an incredibly simple mathematical function that we can treat as an LSH. Define <span class="math inline">\(h_1:{\mathbb{R}}^2 \to {\mathbb{Z}}\)</span> for a point <span class="math inline">\(x=(x_1, x_2)\in{\mathbb{R}}^2\)</span> by</p>
      <p><span class="math display">\[ h_1(x) := \lfloor x_1 \rfloor; \]</span></p>
      <p>that is <span class="math inline">\(h_1(x)\)</span> is the largest integer <span class="math inline">\(a\)</span> for which <span class="math inline">\(a\le x_1.\)</span> For example, <span class="math inline">\(h_1((3.2, -1.2)) = 3.\)</span></p>
      <p>Let’s suppose we choose points at random by uniformly sampling from the origin-centered circle <span class="math inline">\(\mathcal C\)</span> with radius 4:</p>
      <p><span class="math display">\[ \mathcal C := \{ (x, y) : x^2 + y^2 \le 4^2 \}. \]</span></p>
      <p>Suppose we want to find which of our points in <span class="math inline">\(\mathcal C\)</span> are close together. We can estimate this relationship by considering points <span class="math inline">\(a\)</span> and <span class="math inline">\(b \in \mathcal C\)</span> to be clustered together when <span class="math inline">\(h_1(a) = h_1(b).\)</span> It will be handy to introduce the notation <span class="math inline">\(a \sim b\)</span> to indicate that <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are in the same cluster. With that notation, we can write our current hash setup as</p>
      <p><span class="math display">\[ a \sim b \iff h_1(a) = h_1(b). \]</span></p>
      <p>Figure 1 shows an example of such a clustering.</p>
      <div class="figure">
      <p class="caption">Figure 1: Twenty points chosen randomly in a circle with radius 4. Each point <span class="math inline">\(x\)</span> is colored based on its hash value <span class="math inline">\(h_1(x).\)</span></p>
      <img src="images/lsh_image1_v2.png" alt="Figure 1: Twenty points chosen randomly in a circle with radius 4. Each point x is colored based on its hash value h_1(x)." id="fig:fig1" />
      </div>
      <p>You can immediately see that some points are far apart yet clustered, while others are relatively close yet unclustered. There’s also a sense that this particular hash function <span class="math inline">\(h_1\)</span> was arbitrarily chosen to focus on the x-axis. What would have happened with the same data if we had used instead <span class="math inline">\(h_2(x) := \lfloor x_2 \rfloor?\)</span> The result is figure 2.</p>
      <div class="figure">
      <p class="caption">Figure 2: The same twenty points as figure 1, except that we’re using the <span class="math inline">\(y\)</span> values (instead of <span class="math inline">\(x\)</span> values) to determine the hash-based cluster colors this time around.</p>
      <img src="images/lsh_image2.png" alt="Figure 2: The same twenty points as figure 1, except that we’re using the y values (instead of x values) to determine the hash-based cluster colors this time around." id="fig:fig2" />
      </div>
      <p>While neither clustering alone is amazing, things start to work better if we use both of them simultaneously. That is, we can redefine our clustering via</p>
      <p><span id="eq:eq1"><span class="math display">\[ a \sim b \iff h_1(a) = h_1(b) \text{ and } h_2(a) = h_2(b). \qquad(1)\]</span></span></p>
      <p>Our same example points are shown under this new clustering in figure 3.</p>
      <div class="figure">
      <p class="caption">Figure 3: The same twenty points clustered via two different hashes — one using <span class="math inline">\(\lfloor x\rfloor\)</span>, the other using <span class="math inline">\(\lfloor y\rfloor.\)</span> As before, points are colored based on which cluster they’re in; a cluster is the set of all points who share both their <span class="math inline">\(\lfloor x\rfloor\)</span> and their <span class="math inline">\(\lfloor y\rfloor\)</span> values.</p>
      <img src="images/lsh_image3.png" alt="Figure 3: The same twenty points clustered via two different hashes — one using \lfloor x\rfloor, the other using \lfloor y\rfloor. As before, points are colored based on which cluster they’re in; a cluster is the set of all points who share both their \lfloor x\rfloor and their \lfloor y\rfloor values." id="fig:fig3" />
      </div>
      <p>This does a much better job of avoiding clusters with points far apart, although, as we’ll see below, we can still make some improvements.</p>
      <h2 id="randomizing-our-hashes"><span class="header-section-number">2.1</span> Randomizing our hashes</h2>
      <p>So far we’ve defined deterministic hash functions. Let’s change that by choosing a random rotation matrix <span class="math inline">\(U\)</span> (a rotation around the origin) along with a random offset <span class="math inline">\(b \in [0, 1).\)</span> Given such a random <span class="math inline">\(U\)</span> and <span class="math inline">\(b,\)</span> we could define a new hash function via</p>
      <p><span class="math display">\[ h(x) := \lfloor (Ux)_1 + b \rfloor, \]</span></p>
      <p>where I’m using the notation <span class="math inline">\(( \textit{vec} )_1\)</span> to indicate the first coordinate of the vector value <em>vec</em>. (That is, the notation <span class="math inline">\((Ux)_1\)</span> means the first coordinate of the vector <span class="math inline">\(Ux\)</span>.)</p>
      <p>It may seem a tad arbitrary to use only the first coordinate here rather than any other, but the fact that we’re taking a random rotation first means that we have the same set of possibilities, with the same probability distribution, as we would when pulling out any other single coordinate value.</p>
      <p>The advantage of using randomized hash functions is that any theoretical properties we want to discuss will apply without having to worry about pathologically weird data. Conceptually, if we were using deterministic hash functions, then someone could choose the worst-case data for our hash function, and we’d be stuck with that poor performance (for example, choosing maximally-far apart points that are still clustered together by our <span class="math inline">\(h_1\)</span> function above). By using randomly chosen hash functions, we can ensure that any average-case behavior of our hash functions applies equally well to <em>all data</em>. This same perspective is useful for hash tables in the form of <em>universal hashing</em>.</p>
      <p>Let’s revisit the example points we used above, but now apply some randomized hash functions. In figure 3, points are clustered if and only if both of their hash values (from <span class="math inline">\(h_1(x)\)</span> and <span class="math inline">\(h_2(x)\)</span>) collide. We’ll use that same idea, but this time choose four rotations <span class="math inline">\(U_1, \ldots, U_4\)</span> as well as four offsets <span class="math inline">\(b_1, \ldots, b_4\)</span> to define <span class="math inline">\(h_1(), \ldots, h_4()\)</span> via</p>
      <p><span id="eq:eq3"><span class="math display">\[ h_i(x) := \lfloor (U_i x)_1 + b_i \rfloor. \qquad(2)\]</span></span></p>
      <p>Figure 4 shows the resulting clustering. This time, there are 100 points since using more hash functions has effectively made the cluster areas smaller. We need higher point density to see points that are clustered together now.</p>
      <div class="figure">
      <p class="caption">Figure 4: One hundred random points clustered using four random hash functions as defined by (2). Points have the same color when all four of their hash values are the same. Each set of parallel light gray lines delineates the regions with the same hash value for each of the <span class="math inline">\(h_i()\)</span> functions.</p>
      <img src="images/lsh_image4.png" alt="Figure 4: One hundred random points clustered using four random hash functions as defined by (2). Points have the same color when all four of their hash values are the same. Each set of parallel light gray lines delineates the regions with the same hash value for each of the h_i() functions." id="fig:fig4" />
      </div>
      <p>It’s not obvious that we actually want to use all four of our hash functions. The issue is that our clusters have become quite small. There are a couple ways to address this. One is to simply increase the scale of the hash functions; for example, set:</p>
      <p><span class="math display">\[ \tilde h_i(x) := h_i(x/s), \]</span></p>
      <p>where <span class="math inline">\(s\)</span> is a scale factor. In this setup, larger <span class="math inline">\(s\)</span> values will result in larger clusters.</p>
      <p>However, there is something a bit more nuanced we can look at, which is to allow some adaptability in terms of <em>how many hash collisions we require</em>. In other words, suppose we have <span class="math inline">\(k\)</span> total hash functions (just above, we had <span class="math inline">\(k=4\)</span>). Instead of insisting that all <span class="math inline">\(k\)</span> hash values must match before we say two points are in the same cluster, we could look at cases where some number <span class="math inline">\(j \le k\)</span> of them matches. To state this mathematically, we would rewrite equation (1) as</p>
      <p><span id="eq:eq2"><span class="math display">\[ a \sim b \iff \#\{i: h_i(a) = h_i(b)\} \ge j. \qquad(3)\]</span></span></p>
      <p>Something interesting happens here, which is that the <span class="math inline">\(a \sim b\)</span> relationship is no longer a clustering, but becomes more like adjacency (that is, sharing an edge) in a graph. The difference is that, in a clustering, if <span class="math inline">\(a\sim b\)</span> and <span class="math inline">\(b\sim c,\)</span> the we must have <span class="math inline">\(a\sim c\)</span> as well; this is called being <em>transitively closed</em>. Graphs don’t need to have this property, and in our case as well, it’s no longer true that our similarity relationship <span class="math inline">\(a\sim b\)</span> is transitively closed.</p>
      <p>It may help your intuition to see this new definition of <span class="math inline">\(a\sim b\)</span> in action on the same 100 points from figure 4. This time, in figure 5, there are twenty random hashes, and we’re seeing the graphs generated by (3) using cutoff values (values of <span class="math inline">\(j\)</span>) of 6, 7, 8, and 9. The top-left graph in figure 5 has an edge drawn between two points <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> whenever there are at least 6 hash functions <span class="math inline">\(h_i()\)</span> with <span class="math inline">\(h_i(a) = h_i(b),\)</span> out of a possible 20 used hash functions.</p>
      <div class="figure">
      <p class="caption">Figure 5: A set of 100 random points with graph edges drawn according to (3). There are 20 random hash functions used. The top-left graph uses the cutoff value <span class="math inline">\(j=6.\)</span> The remaining three graphs have cutoff values <span class="math inline">\(j=7,\)</span> 8, and 9; this means each graph is a subgraph (having a subset of the edges) of the previous one.</p>
      <img src="images/lsh_image5.png" alt="Figure 5: A set of 100 random points with graph edges drawn according to (3). There are 20 random hash functions used. The top-left graph uses the cutoff value j=6. The remaining three graphs have cutoff values j=7, 8, and 9; this means each graph is a subgraph (having a subset of the edges) of the previous one." id="fig:fig5" />
      </div>
      <p>In fact, we can visualize all possible cutoff values of 6 or higher — these are values of <span class="math inline">\(j\)</span> in equation (3) — using a single image with weighted edges, as seen in figure 6.</p>
      <div class="figure">
      <p class="caption">Figure 6: The same 100 random points from figure 5, this time rendered with edge weights that depend on how many hash collisions are present between any two points. A black edge represents the maximum of 20 hash collisions; the lightest edge represents only 6 hash collisions.</p>
      <img src="images/lsh_image6.png" alt="Figure 6: The same 100 random points from figure 5, this time rendered with edge weights that depend on how many hash collisions are present between any two points. A black edge represents the maximum of 20 hash collisions; the lightest edge represents only 6 hash collisions." id="fig:fig6" />
      </div>
      <p>There’s another fun way to build intuition for what information our hashes provide. Let’s see which regions of our circle are matched, and to what degree, by a given point. We can do this by shading regions of the circle that will match a query point, as in figure 7. Every point in each shaded region has the same hash values for all the hash functions used. The first part of figure 7 shows a scaled version of the two-hash system (using <span class="math inline">\(h_1()\)</span> and <span class="math inline">\(h_2()\)</span>, similar to figure 3) that we saw before; the second part uses 5 random hashes. Call the moving query point <span class="math inline">\(q;\)</span> then any point <span class="math inline">\(p\)</span> in a darkly shaded region will have a hash collision <span class="math inline">\(h_i(p) = h_i(q)\)</span> for all hash functions; in a lightly shaded region that equation will only hold true for a smaller subset of the hash functions <span class="math inline">\(h_i().\)</span></p>
      <p>Imagine that we were drawing these same images for some theoretically perfect LSH setup that somehow managed to match point <span class="math inline">\(q\)</span> to every point <span class="math inline">\(p\)</span> with <span class="math inline">\(||p-q||\le r\)</span> for some radius <span class="math inline">\(r\)</span>; all other points were not matched. For that perfect LSH setup, images like figure 7 would show a fixed-size circle with center at <span class="math inline">\(q\)</span> that moved along with <span class="math inline">\(q\)</span>. With that in mind as the perfect LSH result, notice that the second setting in figure 7 is much closer to this ideal than the first setting. Keep in mind that lookups within the shaded regions are no longer linear searches through data, but rather the intersection of <span class="math inline">\(k\)</span> hash table lookups — that is, lookups of nearby points are significantly faster.</p>
      <div class="figure">
      <p class="caption">Figure 7: The first setting shows the regions where points would be matched by either two (dark regions) or just one (lighter shade) hash collision with the moving query point <span class="math inline">\(q\)</span>. The second setting shows the analogous idea for 5 random hash functions; in the latter case, the lightest shaded region indicates 3 hash collisions.</p>
      <img src="images/image8a@2x.gif" alt="Figure 7: The first setting shows the regions where points would be matched by either two (dark regions) or just one (lighter shade) hash collision with the moving query point q. The second setting shows the analogous idea for 5 random hash functions; in the latter case, the lightest shaded region indicates 3 hash collisions." id="fig:fig8a" />
      </div>
      <p>It may further help your intuition to see how similarity edges, like those in figure 6, change as a single query point moives. This is the idea behind figure 8, where weighted edges are drawn between a moving query point and 100 random points. Notice that the edge weightings make intuitive sense: they tend to connect strongly to very close neighbors, weakly to farther neighbors, and not at all to points beyond a certain distance.</p>
      <div class="figure">
      <p class="caption">Figure 8: Edges weighted by how many hash collisions are present between the moving query point and 100 random points. Darker edges indicate more hash collisions. This image uses 12 random hashes, and requires at least 6 hash collisions for an edge to appear.</p>
      <img src="images/image8b@2x.gif" alt="Figure 8: Edges weighted by how many hash collisions are present between the moving query point and 100 random points. Darker edges indicate more hash collisions. This image uses 12 random hashes, and requires at least 6 hash collisions for an edge to appear." id="fig:fig8b" />
      </div>
      <h2 id="choosing-values-of-j-and-k"><span class="header-section-number">2.2</span> Choosing values of <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span></h2>
      <p>As a quick reminder, <span class="math inline">\(k\)</span> is the number of random hash functions being used, and <span class="math inline">\(j\)</span> is the minimum number of hash collisions you require (out of <span class="math inline">\(k\)</span>) in order to consider points <span class="math inline">\(p, q\)</span> as being nearby, as per (3). I won’t provide a general best answer for the values here, but rather provide some understanding of how these numbers affect the quality of the results you can achieve. Higher <span class="math inline">\(k\)</span> values enable better behavior — they support LSH systems that act more like a radius-cutoff function (that is, more like <span class="math inline">\(p\sim q \iff ||p-q||\le r\)</span> for some radius <span class="math inline">\(r\)</span>); the choice of <span class="math inline">\(j\)</span> is less obvious.</p>
      <p>I wrote a little Python script to estimate the behavior of various <span class="math inline">\(j\)</span> values for <span class="math inline">\(k=12\)</span> in 2 dimensions. I’ve put together an image that I think really captures the quality of these <span class="math inline">\(j\)</span> values, but it takes a bit of explaining up-front.</p>
      <p>Given a value <span class="math inline">\(s \in (0, 1)\)</span>, define the distance <span class="math inline">\(D_s\)</span> as the value satisfying the given equation:</p>
      <p><span id="eq:eq4"><span class="math display">\[ P\big(p \sim_j q \, \big| \, ||p-q|| = D_s\big) = s, \qquad(4)\]</span></span></p>
      <p>where <span class="math inline">\(p\sim_j q\)</span> means that <span class="math inline">\(\#\{i : h_i(p) = h_i(q)\} \ge j.\)</span> Intuitively, if <span class="math inline">\(\varepsilon\)</span> is close to zero, then the distance <span class="math inline">\(D_\varepsilon\)</span> is large because the probability of <span class="math inline">\(p\sim_j q\)</span> is small. The value of <span class="math inline">\(D_{1/2}\)</span> is just the perfect distance so that <span class="math inline">\(p \sim_j q\)</span> happens half of the time, and <span class="math inline">\(D_{1-\varepsilon}\)</span> is a small distance where <span class="math inline">\(p \sim_j q\)</span> happens almost all the time.</p>
      <p>Notice that all our values <span class="math inline">\(D_s\)</span> depend on the dimension we’re in, as well as on <span class="math inline">\(j\)</span> and <span class="math inline">\(k.\)</span> For a fixed value of <span class="math inline">\(k,\)</span> we would typically want a value of <span class="math inline">\(j\)</span> so that <span class="math inline">\(D_\varepsilon / D_{1 - \varepsilon}\)</span> is minimized; as that ratio gets closer to 1 (which is its lower bound), our LSH setup acts more like the ideal radius-cutoff function.</p>
      <p>CHECK update all text (slightly above and below) to be more up-to-date with what image 9 now shows</p>
      <p>We’re ready for figure 9. This is a series of box plots that I’m using to represent the values <span class="math inline">\(D_s\)</span> for <span class="math inline">\(s \in \{\varepsilon, 1/4, 3/4, 1 - \varepsilon\},\)</span> where <span class="math inline">\(\varepsilon = 10^{-5}.\)</span> For each value of <span class="math inline">\(j,\)</span> the <span class="math inline">\(D_s\)</span> values have been divided through by <span class="math inline">\(D_{1/2}\)</span> so that the effective median value is 1. This normalization is important because, without it, it would be much more difficult to visually compare the <em>scales</em> of the ranges for each <span class="math inline">\(j.\)</span> In a normalized setting, the best possible value of <span class="math inline">\(j\)</span> will result in the smallest possible ratio <span class="math inline">\(D_\varepsilon / D_{1 - \varepsilon},\)</span> which we can see visually as a short box plot. In figure 9, the value <span class="math inline">\(j=9\)</span> performs the best.</p>
      <div class="figure">
      <p class="caption">Figure 9: Intuitively, each box plot represents the distances at which points <span class="math inline">\(p, q\)</span> will achieve mixed results (sometimes classified as nearby, other times as not) from our LSH setup. A very short box plot is ideal because it means close points (closer than the bottom whisker) will very likely have <span class="math inline">\(p\sim q\)</span> and far points (farther than the top whisker) will very likely have <span class="math inline">\(p\not\sim q.\)</span> Formally, each box plot shows, from top to bottom, the four values <span class="math inline">\(D_\varepsilon\)</span> (top whisker), <span class="math inline">\(D_{1/4}\)</span> (top of the box), <span class="math inline">\(D_{3/4}\)</span> (bottom of the box), and <span class="math inline">\(D_{1 - \varepsilon}\)</span> (bottom whisker), with these values being each divided through by <span class="math inline">\(D_{1/2}\)</span> so that the effective median of the box plot is at 1. The term <span class="math inline">\(D_s\)</span> was defined in (4).</p>
      <img src="images/image9@2x.png" alt="Figure 9: Intuitively, each box plot represents the distances at which points p, q will achieve mixed results (sometimes classified as nearby, other times as not) from our LSH setup. A very short box plot is ideal because it means close points (closer than the bottom whisker) will very likely have p\sim q and far points (farther than the top whisker) will very likely have p\not\sim q. Formally, each box plot shows, from top to bottom, the four values D_\varepsilon (top whisker), D_{1/4} (top of the box), D_{3/4} (bottom of the box), and D_{1 - \varepsilon} (bottom whisker), with these values being each divided through by D_{1/2} so that the effective median of the box plot is at 1. The term D_s was defined in (4)." id="fig:fig9" />
      </div>
      <h2 id="why-an-lsh-is-faster"><span class="header-section-number">2.3</span> Why an LSH is faster</h2>
      <p>So far we’ve been sticking to 2-dimensional data because that’s easier to visualize in an article. However, if you think about computing 10 hashes for every 2-dimensional point in order to find neighbors, it may feel like you’re doing more work than the simple solution of a linear search through your points. Let’s review cases where using an LSH is more efficient than other methods of finding nearby points.</p>
      <h3 id="zero-linear-search"><span class="header-section-number">2.3.1</span> Zero linear search</h3>
      <p>If you have a huge number <span class="math inline">\(n\)</span> of points, and it’s reasonable for you to index those points ahead of time — meaning, you can afford to compute all <span class="math inline">\(k\)</span> hash values for each point — then you can completely avoid the linear-time cost of a brute force search for nearby points given a new query point. This speed-up is relevant in any dimension, including the simple 2-dimensional case.</p>
      <h3 id="fewer-hashes-needed-in-higher-dimensions"><span class="header-section-number">2.3.2</span> Fewer hashes needed in higher dimensions</h3>
      <p>Another effect that may be less obvious is that you can get away with fewer hash values (a smaller <span class="math inline">\(k\)</span> value) in higher dimensions. There are some mathematically sophisticated ways to quantify that statement, but it may be even easier to understand graph based on empirically derived data.</p>
      <p>Here’s a summary of some random sampling I did in order to explore the relationship between various values of <span class="math inline">\(j\)</span> for <span class="math inline">\(d=100\)</span> dimensional data using <span class="math inline">\(k=10\)</span> different random hashes: CHECK</p>
      <p>CHECK new image here I guess!</p>
      <div class="figure">
      <p class="caption">Figure 10: CHECK description here</p>
      <img src="images/image9@2x.png" alt="Figure 10: CHECK description here" id="fig:fig10" />
      </div>
      <p>CHECK the whole next paragraph What’s interesting here is that we get a relatively tight box plot for <span class="math inline">\(j\)</span> values around CHECK. This means that we can choose the threshold <span class="math inline">\(j=CHECK\)</span> in equation (3) and have fairly good confidence that our hash-based “nearby” relationship closely matches reality.</p>
      <p>We can even quantify this precisely. Although this article doesn’t <em>prove</em> the following implications, the empirical evidence found CHECK(add link to code) strongly suggests that these are in fact the correct values:</p>
      <p><span class="math display">\[ \text{dist}(a, b) &gt; \alpha \Rightarrow P(\#\{i : h_i(a) = h_i(b)\} &lt; j) &gt; 0.95; \]</span></p>
      <p><span class="math display">\[ \text{dist}(a, b) &lt; \beta  \Rightarrow P(\#\{i : h_i(a) = h_i(b)\} \ge j) &lt; 0.05. \]</span></p>
      <p>CHECK(the actual identities may end up being based on the left side using a <span class="math inline">\(j\)</span> value rather than a distance to start with).</p>
      <p>We might interpret these last two expressions as saying that we believe at least 99% of our pairwise relationships are correctly classified. And we’re able to do so while saving about <span class="math inline">\(O(n)\)</span> speed.</p>
      <h2 id="other-data-types-and-approaches"><span class="header-section-number">2.4</span> Other data types and approaches</h2>
      <p>This article has focused on numeric, 2-dimensional data because it’s easier to visualize. Locality-sensitive hashes can certainly be used for many other data types, including strings, sets, or high-dimensional vectors.</p>
      <p>There are also other ways to specifically measure the performance of a particular hashing approach. For example, CHECK.</p>
      <p>Yet another ingredient to throw into the mix here are techniques to boost performance which can treat any LSH as a black box. My favorite approach here is to simply perform multiple lookups on a hash system, each time using <span class="math inline">\(q + \varepsilon\)</span> as an input, where <span class="math inline">\(q\)</span> is your query value, and <span class="math inline">\(\varepsilon\)</span> is a random variable centered at zero. CHECK(add a bit about what this achieves; add a reference for it)</p>
      <p>There’s a lot more that can be said about LSH techniques. If there is reader interest, I may write a follow-up article explaining the details of min-wise hashing, which is a fun case that’s simultaneously good at quickly finding nearby sets as well as nearby strings.</p>
      <p>CHECK ensure that the references section is not numbered</p>
      <hr />
      <h1 id="references"><span class="header-section-number">3</span> References</h1>
    </div>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>CS 229 Homework</title>
    <script src='/Users/tylerneylon/Dropbox/Documents/code/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
    <link rel="stylesheet" href="tufte-edited.css"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
  </head>
  <body>
    <p style="display:none">\(\newcommand{\latexonlyrule}[2]{}\)</p>
    <div class="main">
      <div class="title">CS 229 Homework</div>
      <div class="author">Tyler Neylon</div>
      <div class="date">345.2016</div>
<p>These are solutions to the most recent problems posted for Stanford’s CS 229 course, as of June 2016. I’m not sure if this course re-uses old problems, but please don’t copy the answers if so. This document is also available as a <a href="http://tylerneylon.com/notes/cs229/cs229hw.pdf">pdf</a>.</p>
<h1 id="problem-set-1"><span class="header-section-number">1</span> Problem set 1</h1>
<h2 id="logistic-regression"><span class="header-section-number">1.1</span> Logistic regression</h2>
<h3 id="part-a"><span class="header-section-number">1.1.1</span> Part (a)</h3>
<p>The problem is to compute the Hessian matrix <span class="math inline">\(H\)</span> for the function</p>
<p><span class="math display">\[J({\theta}) = -\frac{1}{m}\sum_{i=1}^m\log(g(y{^{(i)}}x{^{(i)}})),\]</span></p>
<p>where <span class="math inline">\(g(z)\)</span> is the logistic function, and to show that <span class="math inline">\(H\)</span> is positive semi-definite; specifically, that <span class="math inline">\(z^THz\ge 0\)</span> for any vector <span class="math inline">\(z.\)</span></p>
<p>We’ll use the fact that <span class="math inline">\(g&#39;(z) = g(z)(1-g(z)).\)</span> We’ll also note that since all relevant operations are linear, it will suffice to ignore the summation over <span class="math inline">\(i\)</span> in the definition of <span class="math inline">\(J.\)</span> I’ll use the notation <span class="math inline">\(\partial_j\)</span> for <span class="math inline">\(\frac{\partial}{\partial{\theta}_j},\)</span> and introduce <span class="math inline">\(t\)</span> for <span class="math inline">\(y{\theta}^Tx.\)</span> Then</p>
<p><span class="math display">\[-\partial_j(mJ) = \frac{g(t)(1-g(t))}{g(t)}x_jy = x_jy(1-g(t)).\]</span></p>
<p>Next</p>
<p><span class="math display">\[-\partial_k\partial_j(mJ) = x_jy\big(-g(t)(1-g(t))\big)x_ky,\]</span></p>
<p>so that</p>
<p><span class="math display">\[\partial_{jk}(mJ) = x_jx_ky^2\alpha,\]</span></p>
<p>where <span class="math inline">\(\alpha = g(t)(1-g(t)) &gt; 0.\)</span></p>
<p>Thus we can use repeated-index summation notation to arrive at</p>
<p><span class="math display">\[z^THz = z_ih_{ij}z_j = (\alpha y^2)(z_ix_ix_jz_j)
        = (\alpha y^2)(x^Tz)^2 \ge 0.\]</span></p>
<p>This completes this part of the problem.</p>
<h3 id="part-b"><span class="header-section-number">1.1.2</span> Part (b)</h3>
<p>Here is a matlab script to solve this part of the problem:</p>
<pre><code>% problem1_1b.m
%
% Run Newton&#39;s method on a given cost function for a logistic
% regression setup.
%

printf(&#39;Running problem1_1b.m\n&#39;);

% Be able to compute J.
function val = J(Z, theta)
  [m, _] = size(Z);
  g      = 1 ./ (1 + exp(Z * theta));
  val    = -sum(log(g)) / m;
end

% Setup.
X      = load(&#39;logistic_x.txt&#39;);
[m, n] = size(X);
X      = [ones(m, 1) X];
Y      = load(&#39;logistic_y.txt&#39;);
Z      = diag(Y) * X;

% Initialize the parameters to learn.
old_theta =  ones(n + 1, 1);
theta     = zeros(n + 1, 1);
i         = 1;  % i = iteration number.

% Perform Newton&#39;s method.
while norm(old_theta - theta) &gt; 1e-5
  printf(&#39;J = %g\n&#39;, J(Z, theta));
  printf(&#39;theta:\n&#39;);
  disp(theta);
  printf(&#39;Running iteration %d\n&#39;, i);

  g         = 1 ./ (1 + exp(Z * theta));
  f         = (1 - g);
  alpha     = f .* g;
  A         = diag(alpha);
  H         = Z&#39; * A * Z / m;
  nabla     = Z&#39; * f / m;
  old_theta = theta;
  theta     = theta - inv(H) *  nabla;

  i++;
end

% Show and save output.
printf(&#39;Final theta:\n&#39;);
disp(theta);
save(&#39;theta.mat&#39;, &#39;theta&#39;);</code></pre>
<p>Because I have copious free time, I also wrote a Python version. Also because I’m learning numpy and would prefer to consistently use a language that I know can produce decent-looking graphs. Here is the Python script:</p>
<pre><code>#!/usr/bin/env python

import numpy as np
from numpy import linalg as la


# Define the J function.
def J(Z, theta):
  m, _ = Z.shape
  g    = 1 / (1 + np.exp(Z.dot(theta)))
  return -sum(np.log(g)) / m

# Load data.
X    = np.loadtxt(&#39;logistic_x.txt&#39;)
m, n = X.shape
X    = np.insert(X, 0, 1, axis=1)  # Prefix an all-1 column.
Y    = np.loadtxt(&#39;logistic_y.txt&#39;)
Z    = np.diag(Y).dot(X);

# Initialize the learning parameters.
old_theta = np.ones((n + 1,))
theta     = np.zeros((n + 1,))
i         = 1

# Perform Newton&#39;s method.
while np.linalg.norm(old_theta - theta) &gt; 1e-5:

  # Print progress.
  print(&#39;J = {}&#39;.format(J(Z, theta)))
  print(&#39;theta = {}&#39;.format(theta))
  print(&#39;Running iteration {}&#39;.format(i))

  # Update theta.
  g         = 1 / (1 + np.exp(Z.dot(theta)))
  f         = 1 - g
  alpha     = (f * g).flatten()
  H         = (Z.T * alpha).dot(Z) / m
  nabla     = Z.T.dot(f) / m
  old_theta = theta
  theta     = theta - la.inv(H).dot(nabla)

  # Update i = the iteration counter.
  i += 1

# Print and save the final value.
print(&#39;Final theta = {}&#39;.format(theta))
np.savetxt(&#39;theta.txt&#39;, theta)</code></pre>
<p>The final value of <span class="math inline">\({\theta}\)</span> that I arrived at is</p>
<p><span class="math display">\[{\theta}= (2.62051, -0.76037, -1.17195).\]</span></p>
<p>The first value <span class="math inline">\({\theta}_0\)</span> represents the constant term, so that the final model is given by</p>
<p><span class="math display">\[y = g(2.62 - 0.76x_1 - 1.17x_2).\]</span></p>
<h3 id="part-c"><span class="header-section-number">1.1.3</span> Part (c)</h3>
<div class="figure">
<p class="caption">The data points given for problem 1.1 along with the decision boundary learned by logistic regression as executed by Newton’s method.</p>
<img src="images/pr1_1c.png" alt="The data points given for problem 1.1 along with the decision boundary learned by logistic regression as executed by Newton’s method." />
</div>
<h2 id="poisson-regression-and-the-exponential-family"><span class="header-section-number">1.2</span> Poisson regression and the exponential family</h2>
<h3 id="part-a-1"><span class="header-section-number">1.2.1</span> Part (a)</h3>
<p>Write the Poisson distribution as an exponential family:</p>
<p><span class="math display">\[p(y;\eta) = b(y)\exp\big(\eta^T T(y) - a(\eta)\big),\]</span></p>
<p>where</p>
<p><span class="math display">\[p(y;\lambda) = \frac{e^{-\lambda}\lambda^y}{y!}.\]</span></p>
<p>This can be done via</p>
<p><span class="math display">\[\begin{array}{rcl}
\eta &amp; = &amp; \log(\lambda), \\
a(\eta) &amp; = &amp; e^\eta = \lambda, \\
b(y) &amp; = &amp; 1/y!, \text{ and} \\
T(y) &amp; = &amp; y.
\end{array}\]</span></p>
<h3 id="part-b-1"><span class="header-section-number">1.2.2</span> Part (b)</h3>
<p>As is usual with generalized linear models, we’ll let <span class="math inline">\(\eta = {\theta}^Tx.\)</span> The canonical response function is then given by</p>
<p><span class="math display">\[g(\eta) = E[y;\eta] = \lambda = e^\eta = e^{{\theta}^Tx}.\]</span></p>
<h3 id="part-c-1"><span class="header-section-number">1.2.3</span> Part (c)</h3>
<p>Based on the last part, I’ll define the hypothesis function <span class="math inline">\(h\)</span> via <span class="math inline">\(h(x) = e^{{\theta}^Tx}.\)</span></p>
<p>For a single data point <span class="math inline">\((x, y),\)</span> let <span class="math inline">\(\ell({\theta}) = \log(p(y|x)) =\)</span> <span class="math inline">\(\log(\frac{1}{y!}) + (y{\theta}^T x-e^{{\theta}^Tx}).\)</span> Then</p>
<p><span class="math display">\[\frac{\partial}{\partial{\theta}_j}\ell({\theta}) = yx_j - x_je^{{\theta}^Tx}
= x_j(y-e^{{\theta}^Tx}).\]</span></p>
<p>So stochastic gradient ascent for a single point <span class="math inline">\((x, y)\)</span> would use the update rule</p>
<p><span class="math display">\[{\theta}:= {\theta}+ \alpha x(y - h(x)).\]</span></p>
<h3 id="part-d"><span class="header-section-number">1.2.4</span> Part (d)</h3>
<p>In section 1.10 of my notes — the section on generalized linear models — I derived the update rule:</p>
<p><span class="math display">\[{\theta}:= {\theta}+ \alpha\big(T(y)-a&#39;({\theta}^Tx)\big)x.\]</span></p>
<p>The missing piece is to proof that <span class="math inline">\(h(x) = E[y] = a&#39;(\eta),\)</span> which we’ll do next. We’ll work in the context of <span class="math inline">\(T(y)=y,\)</span> as given by the problem statement. Notice that, for any <span class="math inline">\(\eta,\)</span></p>
<p><span class="math display">\[\int p(y)dy = \int b(y)\exp(\eta^Ty - a(\eta))dy = 1.\]</span></p>
<p>Since this identity is true for all values of <span class="math inline">\(\eta,\)</span> we can take <span class="math inline">\(\frac{\partial}{\partial\eta}\)</span> of it to arrive at the value 0:</p>
<p><span class="math display">\[\begin{array}{rcl}
 0 &amp; = &amp; \frac{\partial}{\partial\eta}\int p(y)dy \\
   &amp; = &amp; \int \frac{\partial}{\partial\eta} b(y)\exp(\eta^Ty - a(\eta))dy \\
   &amp; = &amp; \int b(y)(y - a&#39;(\eta))\exp(\eta^Ty - a(\eta)) dy \\
   &amp; = &amp; \int y p(y)dy - a&#39;(\eta)\int p(y)dy \\
   &amp; = &amp; E[y] - a&#39;(\eta).
\end{array}\]</span></p>
<p>Thus we can conclude that <span class="math inline">\(E[y] = a&#39;(\eta) = a&#39;({\theta}^Tx),\)</span> which completes the solution.</p>
<h2 id="gaussian-discriminant-analysis"><span class="header-section-number">1.3</span> Gaussian discriminant analysis</h2>
<h3 id="part-a-2"><span class="header-section-number">1.3.1</span> Part (a)</h3>
<p>This problem is to show that a two-class GDA solution effectively provides a model that takes the form of a logistic function, similar to logistic regression. This is something I already did in section 2.1 of <a href="http://tylerneylon.com/notes/cs229/cs229.html">my notes</a>.</p>
    </div>
  </body>
</html>

\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{unicode=true,
            pdftitle={Notes on Andrew Ng's CS 229 Machine Learning Course},
            pdfauthor={Tyler Neylon},
            colorlinks=true,
            linkcolor=black,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\usepackage{subfig}
\AtBeginDocument{%
\renewcommand*\figurename{Figure}
\renewcommand*\tablename{Table}
}
\AtBeginDocument{%
\renewcommand*\listfigurename{List of Figures}
\renewcommand*\listtablename{List of Tables}
}
\usepackage{float}
\floatstyle{ruled}
\makeatletter
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\makeatother
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}

\title{Notes on Andrew Ng's CS 229 Machine Learning Course}
\author{Tyler Neylon}
\date{331.2016}

% Begin custom, non-pandoc commands.

\newcommand{\latexonlyrule}{\rule}

% End custom, non-pandoc commands.

\begin{document}
\maketitle

\newcommand{\R}{\mathbb{R}}
\newcommand{\eqnset}[1]{\left.\mbox{$#1$}\quad\quad\right\rbrace}
\newcommand{\tr}{\text{tr}\;}

These are notes I'm taking as I review material from Andrew Ng's CS 229
course on machine learning. Specifically, I'm watching
\href{https://www.youtube.com/view_play_list?p=A89DCFA6ADACE599}{these
videos} and looking at the written notes and assignments posted
\href{http://cs229.stanford.edu/materials.html}{here}. These notes are
available in two formats:
\href{http://tylerneylon.com/notes/cs229/cs229.html}{html} and
\href{http://tylerneylon.com/notes/cs229/cs229.pdf}{pdf}.

I'll organize these notes to correspond with the written notes from the
class.

\section{On lecture notes 1}\label{on-lecture-notes-1}

The notes in this section are based on
\href{http://cs229.stanford.edu/notes/cs229-notes1.pdf}{lecture notes
1}.

\subsection{Gradient descent in
general}\label{gradient-descent-in-general}

Given a cost function \(J(\theta)\), the general form of an update is

\[\theta_j := \theta_j - \alpha\frac{\partial J}{\partial \theta_j}.\]

It bothers me that \(\alpha\) is an arbitrary parameter. What is the
best way to choose this parameter? Intuitively, it could be chosen based
on some estimate or actual value of the second derivative of \(J\). What
can be theoretically guaranteed about the rate of convergence under
appropriate conditions?

Why not use Newton's method? A general guess: the second derivative of
\(J\) becomes cumbersome to work with.

It seems worthwhile to keep my eye open for opportunities to apply
improved optimization algorithms in specific cases.

\subsection{Gradient descent on linear
regression}\label{gradient-descent-on-linear-regression}

I realize this is a toy problem because linear regression in practice is
not solve iteratively, but it seems worth understanding well. The
general update equation is, for a single example \(i\),

\[\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}.\]

The delta makes sense in that it is proportional to the error
\(y-h_\theta\), and in that the sign of the product \((y-h_\theta)x\)
guarantees moving in the right direction. However, my first guess would
be that the expression \((y-h_\theta)/x\) would provide a better update.

For example, suppose we have a single data point \((x, y)\) where
\(x\ne 0\), and a random value of \(\theta\). Then a great update would
be

\[\theta_1 := \theta_0 + (y - \theta_0 x)/x,\]

since the next hypothesis value \(h_\theta\) would then be

\[h_\theta = \theta_1 x = \theta_0 x + y - \theta_0x = y,\]

which is good. Another intuitive perspective is that we should be making
\emph{bigger} changes to \(\theta_j\) when \(x_j\) is \emph{small},
since it's harder to influence \(h_\theta\) for such \(x\) values.

This is not yet a solidified intuition. I'd be interested in revisiting
this question if I have time.

\subsection{Persistence of trace}\label{persistence-of-trace}

The trace of a square matrix obeys the nice property that

\begin{equation}\tr AB = \tr BA.\label{eq:persistent_trace}\end{equation}

One way to see this is to note that

\[\tr AB = a_{ij}b_{ji} = \tr BA,\]

where I'm using the informal shorthand notation that a variable repeated
within a single product implies that the sum is taken over all relevant
values of that variable. Specifically,

\[a_{ij}b_{ji} \;\text{means}\; \sum_{i,j} a_{ij}b_{ji}.\]

I wonder if there's a more elegant way to verify
(\ref{eq:persistent_trace}).

This notation will become more useful in a moment.

Ng gives other interesting trace-based equations, examined next.

\[\text{Goal:}\quad \nabla_A\tr AB = B^T.\]

Since

\[\tr AB = a_{ij}b_{ji},\]

we have that

\[(\nabla_A\tr AB)_{ij} = b_{ji},\]

verifying the goal.

\[\text{Goal:}\quad \nabla_{A^T}f(A) = (\nabla_A f(A))^T.\]

TODO continue from here

\hypertarget{refs}{}

\end{document}

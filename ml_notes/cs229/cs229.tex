\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{unicode=true,
            pdftitle={Notes on Andrew Ng's CS 229 Machine Learning Course},
            pdfauthor={Tyler Neylon},
            colorlinks=true,
            linkcolor=black,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\usepackage{subfig}
\AtBeginDocument{%
\renewcommand*\figurename{Figure}
\renewcommand*\tablename{Table}
}
\AtBeginDocument{%
\renewcommand*\listfigurename{List of Figures}
\renewcommand*\listtablename{List of Tables}
}
\usepackage{float}
\floatstyle{ruled}
\makeatletter
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\makeatother
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}

\title{Notes on Andrew Ng's CS 229 Machine Learning Course}
\author{Tyler Neylon}
\date{331.2016}

% Begin custom, non-pandoc commands.

\newcommand{\latexonlyrule}{\rule}

% End custom, non-pandoc commands.

\begin{document}
\maketitle

\newcommand{\R}{\mathbb{R}}
\newcommand{\eqnset}[1]{\left.\mbox{$#1$}\quad\quad\right\rbrace}
\newcommand{\tr}{\text{tr}\;}

These are notes I'm taking as I review material from Andrew Ng's CS 229
course on machine learning. Specifically, I'm watching
\href{https://www.youtube.com/view_play_list?p=A89DCFA6ADACE599}{these
videos} and looking at the written notes and assignments posted
\href{http://cs229.stanford.edu/materials.html}{here}. These notes are
available in two formats:
\href{http://tylerneylon.com/notes/cs229/cs229.html}{html} and
\href{http://tylerneylon.com/notes/cs229/cs229.pdf}{pdf}.

I'll organize these notes to correspond with the written notes from the
class.

As I write these notes, I'm also putting together some
\href{http://tylerneylon.com/notes/cs229/cs229hw.html}{homework
solutions}.

\section{On lecture notes 1}\label{on-lecture-notes-1}

The notes in this section are based on
\href{http://cs229.stanford.edu/notes/cs229-notes1.pdf}{lecture notes
1}.

\subsection{Gradient descent in
general}\label{gradient-descent-in-general}

Given a cost function \(J(\theta)\), the general form of an update is

\[\theta_j := \theta_j - \alpha\frac{\partial J}{\partial \theta_j}.\]

It bothers me that \(\alpha\) is an arbitrary parameter. What is the
best way to choose this parameter? Intuitively, it could be chosen based
on some estimate or actual value of the second derivative of \(J\). What
can be theoretically guaranteed about the rate of convergence under
appropriate conditions?

Why not use Newton's method? A general guess: the second derivative of
\(J\) becomes cumbersome to work with.

It seems worthwhile to keep my eye open for opportunities to apply
improved optimization algorithms in specific cases.

\subsection{Gradient descent on linear
regression}\label{gradient-descent-on-linear-regression}

I realize this is a toy problem because linear regression in practice is
not solve iteratively, but it seems worth understanding well. The
general update equation is, for a single example \(i\),

\[\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}.\]

The delta makes sense in that it is proportional to the error
\(y-h_\theta\), and in that the sign of the product \((y-h_\theta)x\)
guarantees moving in the right direction. However, my first guess would
be that the expression \((y-h_\theta)/x\) would provide a better update.

For example, suppose we have a single data point \((x, y)\) where
\(x\ne 0\), and a random value of \(\theta\). Then a great update would
be

\[\theta_1 := \theta_0 + (y - \theta_0 x)/x,\]

since the next hypothesis value \(h_\theta\) would then be

\[h_\theta = \theta_1 x = \theta_0 x + y - \theta_0x = y,\]

which is good. Another intuitive perspective is that we should be making
\emph{bigger} changes to \(\theta_j\) when \(x_j\) is \emph{small},
since it's harder to influence \(h_\theta\) for such \(x\) values.

This is not yet a solidified intuition. I'd be interested in revisiting
this question if I have time.

\subsection{Properties of the trace
operator}\label{properties-of-the-trace-operator}

The trace of a square matrix obeys the nice property that

\begin{equation}\tr AB = \tr BA.\label{eq:persistent_trace}\end{equation}

One way to see this is to note that

\[\tr AB = a_{ij}b_{ji} = \tr BA,\]

where I'm using the informal shorthand notation that a variable repeated
within a single product implies that the sum is taken over all relevant
values of that variable. Specifically,

\[a_{ij}b_{ji} \;\text{means}\; \sum_{i,j} a_{ij}b_{ji}.\]

I wonder if there's a more elegant way to verify
(\ref{eq:persistent_trace}).

Ng gives other interesting trace-based equations, examined next.

\begin{itemize}
\tightlist
\item
  Goal: \(\quad \nabla_A\tr AB = B^T.\)
\end{itemize}

Since

\[\tr AB = a_{ij}b_{ji},\]

we have that

\[(\nabla_A\tr AB)_{ij} = b_{ji},\]

verifying the goal.

\begin{itemize}
\tightlist
\item
  Goal: \(\quad \nabla_{A^T}f(A) = (\nabla_A f(A))^T.\)
\end{itemize}

This can be viewed as

\[(\nabla_{A^T}f(A))_{ij} = \frac{\partial f}{\partial a_{ji}}
                          = (\nabla_A f(A))_{ji}.\]

\begin{itemize}
\tightlist
\item
  Goal: \(\quad \nabla_A\text{tr}(ABA^TC) = CAB + C^TAB^T.\)
\end{itemize}

I'll use some nonstandard index variable names below because I think it
will help avoid possible confusion. Start with

\[(ABA^TC)_{xy} = a_{xz} b_{zw} a_{vw} c_{vy}.\]

Take the trace of that to arrive at

\[\alpha = \text{tr}(ABA^TC) = a_{xz} b_{zw} a_{vw} c_{vx}.\]

Use the product rule to find \(\frac{\partial\alpha}{\partial a_{ij}}\).
You can think of this as, in the equation above, first setting
\(xz = ij\) for one part of the product rule output, and then setting
\(vw = ij\) for the other part. The result is

\[(\nabla_A\alpha)_{ij} = b_{jw} a_{vw} c_{vi} + a_{xz} b_{zj} c_{ix}
                        = c_{vi} a_{vw} b_{jw} + c_{ix} a_{xz} b_{zj}.\]

(The second equality above is based on the fact that we're free to
rearrange terms within products in the repeated-index notation being
used. Such rearrangement is commutativity of numbers, not of matrices.)

This last expression is exactly the \(ij^\text{th}\) entry of the matrix
\(CAB + C^TAB^T\), which was the goal.

\subsection{Derivation of the normal
equation}\label{derivation-of-the-normal-equation}

Ng starts with

\[\nabla_\theta J(\theta) = \nabla_\theta\frac{1}{2}(X\theta-y)^T(X\theta-y),\]

and uses some trace tricks to get to

\[X^TX\theta - X^Ty.\]

I thought that the trace tricks were not great in the sense that if I
were faced with this problem it would feel like a clever trick out of
thin air to use the trace (perhaps due to my own lack of experience with
matrix derivatives?), and in the sense that the connection between the
definition of \(J(\theta)\) and the result is not clear.

Next is another approach.

Start with
\(\nabla_\theta J(\theta) = \nabla_\theta \frac{1}{2}(\theta^TZ\theta - 2y^TX\theta);\)
where \(Z=X^TX\), and the doubled term is a valid combination of the two
similar terms since they're both real numbers, so we can safely take the
transpose of one of them to add them together.

\newcommand{\nt}{\nabla_\theta}

The left term, \(\theta^T Z\theta\), can be expressed as
\(w=\theta_{i1}Z_{ij}\theta_{j1}\), treating \(\theta\) as an
\((n+1)\times 1\) matrix. Then
\((\nt w)_{i1} = Z_{ij}\theta_{j1} + \theta_{j1}Z_{ji}\), using the
product rule; so \(\nt w = 2Z\theta\), using that \(Z\) is symmetric.

The right term, \(v = y^TX\theta = y_{i1}X_{ij}\theta_{j1}\) with
\((\nt v)_{i1} = y_{j1}X_{ji}\) so that \(\nt v = X^Ty\).

These all lead to \(\nt J(\theta) = X^TX\theta - X^Ty\) as before. I
think it's clearer, though, once you grok the sense that

\[\begin{array}{rcl}
\nt \theta^TZ\theta & = & Z\theta + (\theta^T Z)^T, \text{and} \\
\nt u^T\theta & = & u, \\
\end{array}\]

both as intuitively straightforward matrix versions of derivative
properties.

I suspect this can be made even cleaner since the general product rule

\[\nabla(f\cdot g) = (\nabla f)\cdot g + f \cdot (\nabla g)\]

holds, even in cases where the product \(fg\) is not a scalar; e.g.,
that it is vector- or matrix-valued. But I'm not going to dive into that
right now.

Also note that \(X^TX\) can easily be singular. A simple example is
\(X=0\), the scalar value. However, if \(X\) is \(m\times n\) with rank
\(n\), then \(X^TXe_i = X^Tx^{(i)} \ne 0\) since
\(\langle x^{(i)}, x^{(i)}\rangle \ne 0.\) (If
\(\langle x^{(i)}, x^{(i)}\rangle = 0\) then \(X\) could not have rank
\(n.\)) So \(X^TX\) is nonsingular iff \(X\) has rank \(n\).

Ng says something in a lecture (either 2 or 3) that implied that
\((X^TX)^{-1}X^T\) is \emph{not} the pseudoinverse of \(X\), but for any
real-valued full-rank matrix, it \emph{is}.

\subsection{A probabilistic motivation for least
squares}\label{a-probabilistic-motivation-for-least-squares}

This motivation for least squares makes sense when the error is given by
i.i.d. normal curves, and often this may seem like a natural assumption
based on the central limit theorem.

However, this same line of argument could be used to justify any cost
function of the form

\[J(\theta) = \sum_i f(\theta, x^{(i)}, y^{(i)}),\]

where \(f\) is intuitively some measure of distance between
\(h_\theta(x^{(i)})\) and \(y^{(i)}\). Specifically, model the error
term \(\varepsilon^{(i)}\) as having the probability density function
\(e^{-f(\theta, x^{(i)}, y^{(i)})}\). This is intuitively reasonable
when \(f\) has the aforementioned distance-like property, since error
values are likely near zero and unlikely far from zero. Then the log
likelihood function is

\[\ell(\theta) = \log L(\theta)
               = \log \prod_i e^{-f(\theta, x^{(i)}, y^{(i)})}
               = \sum_i -f(\theta, x^{(i)}, y^{(i)}),\]

so that maximizing \(L(\theta)\) is the same as minimizing the
\(J(\theta)\) defined in terms of this arbitrary function \(f.\) To be
perfectly clear, the motivation Ng provides only works when you have
good reason to believe the error terms are indeed normal. At the same
time, using a nice simple algorithm like least squares is quite
practical even if you don't have a great model for your error terms.

\subsection{Locally weighted linear regression
(LWR)}\label{locally-weighted-linear-regression-lwr}

This idea is that, given a value \(x\), we can choose \(\theta\) to
minimize the modified cost function

\[\sum_i w^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2,\]

where

\[w^{(i)} = \exp\left(-\frac{(x^{(i)}-x)^2}{2\tau^2}\right).\]

I wonder: Why not just compute the value

\[y = \frac{\sum_i w^{(i)}y^{(i)}}{\sum_i w^{(i)}}\]

instead?

I don't have a strong intuition for which approach would be better,
although the linear interpolation is more work (unless I'm missing a
clever way to implement LWR that wouldn't also work for the simpler
equation immediately above). This also reminds me of
\href{https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm}{the
\(k-\)nearest neighbors algorithm}, though I've seen that presented as a
classification approach while LWR is regression. Nonetheless, perhaps
one could apply a locality-sensitive hash to quickly approximately find
the \(k\) nearest neighbors and then build a regression model using
that.

\subsection{Logistic regression}\label{logistic-regression}

This approach uses a hypothesis function of the form

\renewcommand{\th}{\theta}

\[h_\th(x) = g(\th^Tx),\]

where

\[g(z) = 1/(1+e^{-z}).\]

The update equation from gradient descent turns out to be nice for this
setup. However, besides ``niceness,'' it's not obvious to me why the
logistic function \(g\) is signifcantly better than any other sigmoid
function.

In general, suppose

\[h_\th(x) = \tau(\th^Tx),\]

where \(\tau\) is any sigmoid function. Then

\newcommand{\toi}{^{(i)}}

\[\left[\nabla_\th\ell(\th)\right]_j
= \sum_i\left(\frac{y\toi}{h\toi} - \frac{1-y\toi}{1-h\toi}\right)
\frac{\partial\tau\toi}{\partial\th_j}.\]

Here, \(\tau\toi\) indicates the function \(\tau\) evaluated on \(\th\)
and \(x\toi\). We can split up the term inside the sum like so:

\[a\toi = 
\frac{y\toi}{h\toi} - \frac{1-y\toi}{1-h\toi}
= \frac{y\toi - h\toi}{h\toi(1-h\toi)},\]

and

\[b\toi = 
\frac{\partial\tau\toi}{\partial\th_j}.\]

Intuitively, the value of \(a\toi\) makes sense as it's the error of
\(h\toi\) weighted more heavily when a wrong output value of \(h\) is
given with higher confidence. The value \(b\toi\) makes sense as the
direction in which we'd want to move \(\th_j\) in order to increase
\(\tau\toi\). Multiplied by the sign of \(a\toi\), the end result is a
movement of \(\th_j\) that decreases the error.

It's not obvious to me which choice of function \(\tau\) is best.
Consider, for example, the alternative function

\[\tau(z) = \frac{\arctan(z)}{\pi} + \frac{1}{2}.\]

In this case, when \(z=\th^Tx\),

\[b\toi
= \frac{\partial\tau\toi}{\partial\th_j} = \frac{x_j\toi}{1+(\th^Tx)^2}.\]

In general, sigmoid functions will tend to give a value of \(b\toi\)
that is small when \(|\th^Tx|\) is large, symmetric around \(\th^Tx=0\),
and has the same sign as \(x_j\) since \(\tau - 1/2\) will be an
increasing odd function; the derivative of an increasing odd function is
even and positive; and this positive even function will be multiplied by
\(x_j\) to derive the final value of \(b\toi.\) The values of \(b\toi\)
will be small for large \(|\th^Tx|\) because \(\tau(\th^Tx)\)
necessarily flattens out for large input values (since it's monotonic
and bounded in \([0,1]\)).

Ng states that the assumption of a Bernoulli distribution for
\(p(y|x;\th)\) results in the traditional gradient update rule for
logistic regression, but there is another choice available in the
argument. The definition of a Bernoulli distribution means that we must
have

\[p(y|x;\th) = \begin{cases}
h_\th(x) & \text{if } y = 1, \text{ and} \\
1 - h_\th(x) & \text{otherwise}.
\end{cases}\]

There are a number of expressions that also capture this same behavior
for the two values \(y=0\) and \(y=1.\) The one used in the course is

\[p(y|x;\th) = h^y(1-h)^{1-y},\]

which is convenient since we're working with the product
\(p(y) = \prod_i p(y\toi).\) However, we could have also worked with

\[p(y|x;\th) = yh + (1-y)(1-h),\]

which also gives exactly the needed values for the cases \(y=1\) and
\(y=0.\) Although the traditional expression is easier to work with, it
seems worth nothing that there is a choice to be made.

\subsection{Using Newton's method for logistic
regression}\label{using-newtons-method-for-logistic-regression}

Ng provides the alternative logistic regression update formula

\begin{equation}\th := \th - H^{-1}\nabla_\th\ell(\th)\label{eq:vector_newton}\end{equation}

without justifying it beyond explaining that it's the vector version of
what one would do in a non-vector setting for finding an optima of
\(\ell'(\th),\) namely,

\[\th := \th - \frac{\ell'(\th)}{\ell''(\th)}.\]

The \(H\) in the above equation is the \emph{Hessian} of \(\ell\), which
I'll write as

\[H = (h_{ij}) =
\left(\frac{\partial^2\ell}{\partial\th_i\partial\th_j}\right).\]

Here is one way to arrive at (\ref{eq:vector_newton}):

\begin{equation}\big[\nabla\ell(\th+\alpha)\big]_i
  \approx \big[\nabla\ell(\th)\big]_i
  + \sum_j\frac{\partial}{\partial\th_j}\big[\nabla\ell(\th)\big]_i
  \cdot\alpha_j.\label{eq:lin_nab_ell}\end{equation}

This is the linearized approximation of \(\nabla\ell\) around \(\th\);
another perspective is to see this as the first-order Taylor polynomial
of \(\big[\nabla\ell\big]_i\) around \(\th\).

We can rewrite (\ref{eq:lin_nab_ell}) more concisely as

\begin{equation}\nabla\ell(\th + \alpha) \approx \nabla\ell(\th) + H\alpha.\label{eq:concise_lin_ell}\end{equation}

Recall that our goal is to iterate on \(\th\) toward a solution to
\(\nabla\ell(\th)=0\). Use (\ref{eq:concise_lin_ell}) toward this end by
looking for \(\alpha\) which solves
\(\nabla\ell(\th + \alpha) \approx 0.\) If \(H\) is invertible, then we
have

\newcommand{\lrarr}{\quad\Leftrightarrow\quad}

\[\nabla\ell(\th) + H\alpha = 0
\lrarr H\alpha = -\nabla\ell(\th)
\lrarr \alpha = -H^{-1}\nabla\ell(\th).\]

In other words, the desired iteration is

\[\th := \th + \alpha = \th - H^{-1}\nabla\ell(\th),\]

which confirms the goal, equation (\ref{eq:vector_newton}).

\subsection{The exponential family}\label{the-exponential-family}

A class of distributions fits within the exponential family when there
are functions \(a,\) \(b,\) and \(T\) such that

\begin{equation}p(y;\eta) = b(y)\exp(\eta^TT(y)-a(\eta))\label{eq:exp_fam}\end{equation}

expresses a distribution in the class.

Ng uses this form to justify the particular use of
\(g(z) = 1/(1+e^{-z})\) in logistic regression. I see how expressing a
Bernoulli distribution in this form naturally result in the traditional
form of logistic regression. However, as of the end of lecture 4, I
don't think Ng has justified the power of the exponential family beyond
explaining that it captures many popular distribution classes. (To be
clear, I believe that this idea is very useful; I just am not convinced
that \emph{why} it's useful has been explicitly explained yet.)

I haven't fully grokked the exponential family yet, but I noticed that
the main restriction in expression (\ref{eq:exp_fam}) seems to be that
the only interaction between \(\eta\) and \(y\) is multiplicative in the
sense that we could equally-well have written the density function as

\[p(y;\eta) = b(y)c(\eta)\prod_i d_i(y_i)^{\eta_i},\]

in the case that both \(\eta\) and \(y\) are vectors.

\subsection{Generalized linear models}\label{generalized-linear-models}

Ng doesn't spell this out, but I think the following line of argument is
why generalized linear models are considered useful.

Suppose \(\eta = \th^Tx\) and that we're working with a distribution
parametrized by \(\eta\) that can be expressed as

\[p(y;\eta) = b(y)\exp\big(\eta^TT(y)-a(y)\big).\]

Then the log likelihood function is given by

\[\ell(\th) = \sum_i\log\big(p(y\toi;\eta\toi)\big)
= \sum_i\log(b(y\toi)) + \eta^{(i)T}T(y\toi)-a(\eta\toi).\]

We can express the gradient of this as

\[\nabla\ell(\th) = \sum_i x\toi\big(T(y\toi)-a'(\eta\toi)\big),\]

which corresponds to the gradient ascent update rule

\begin{equation}\th := \th + \alpha\big(T(y)-a'(\th^Tx)\big)x.\label{eq:glm_gradient_update}\end{equation}

Let's see how this nice general update rule looks in a couple special
cases. For simplicity, I'll stick to special cases where \(\eta\) is a
scalar value.

\subsubsection{Gaussian GLM}\label{gaussian-glm}

In this case, \(a(\eta) = \eta^2/2\) so \(a'(\eta) = \eta\); and
\(T(y)=y\). Then the update rule (\ref{eq:glm_gradient_update}) becomes

\[\th := \th + \alpha(y-\th^Tx)x,\]

which is just gradient ascent applied to least squares.

\subsubsection{Logistic regression GLM}\label{logistic-regression-glm}

In this case, \(a(\eta) = \log(1 + e^\eta)\) and \(T(y)=y\). So
\(a'(\eta) = 1/(1+e^{-\eta})\), and we can write the update rule
(\ref{eq:glm_gradient_update}) as

\[\th := \th + \alpha(y - g(\th^Tx))x,\]

where \(g\) is the logistic function. This matches the earlier update
equation we saw for logistic regression, suggesting that our general
approach isn't completely insane. It may even be technically correct.

\section{On lecture notes 2}\label{on-lecture-notes-2}

The notes in this section are based on
\href{http://cs229.stanford.edu/notes/cs229-notes2.pdf}{lecture notes
2}.

\subsection{Why Gaussian discriminant analysis is like logistic
regression}\label{why-gaussian-discriminant-analysis-is-like-logistic-regression}

Ng mentions this fact in the lecture and in the notes, but he doesn't go
into the details of justifying it, so let's do that.

The goal is to show that

\begin{equation}p(y=1\mid x) = \frac{1}{1+e^{-\th^T x}},\label{eq:gda_goal}\end{equation}

where \(\th\) is some function of the learned parameters \(\phi,\)
\(\Sigma,\) \(\mu_0,\) and \(\mu_1;\) and we can consider \(x\) as being
augmented by a new coordinate \(x_0=1\) to effectively allow the
addition of a constant in the expression \(\th^Tx.\) In Gaussian
discriminant analysis, we learn a model for \(p(x\mid y)\) and for
\(p(y).\) Specifically,

\[\begin{array}{rcl}
p(y) & = & \phi^y(1-\phi)^{1-y}, \\
p(x\mid y=0) & = & N(\mu_0, \Sigma), \text{ and} \\
p(x\mid y=1) & = & N(\mu_1, \Sigma), \\
\end{array}\]

where \(N(\mu,\Sigma)\) indicates the multivariate normal distribution.
From this we can derive

\begin{equation}p(y=1\mid x) = p(x\mid y=1)p(y=1)/p(x).\label{eq:gda_part1}\end{equation}

For \(j=0,1,\) let \(A_j=p(x\mid y=j).\) Then we can write

\[p(x) = p(x\mid y=0)p(y=0) + p(x\mid y=1)p(y=1) = A_0(1-\phi) + A_1\phi.\]

Plug this expression for \(p(x)\) back into (\ref{eq:gda_part1}) to get

\begin{equation}p(y=1\mid x) = \frac{\phi A_1}{(1-\phi)A_0 + \phi A_1}
= \frac{1}{1 + \frac{A_0}{A_1}\left(\frac{1-\phi}{\phi}\right)}.\label{eq:gda_part2}\end{equation}

At this point it will be useful to introduce the shorthand notation

\[\langle x, y\rangle := x^T\Sigma^{-1}y,\]

noting that this is linear in both terms just as the usual inner product
is. It will also be convenient to define

\[B_j = \langle x-\mu_j, x-\mu_j\rangle
      = \langle x, x\rangle - 2\langle \mu_j,x\rangle + \langle \mu_j, \mu_j
      \rangle.\]

Our ultimate goal (\ref{eq:gda_goal}) can now be reduced to showing that
the denominator of (\ref{eq:gda_part2}) is of the form
\(1 + \exp(a\langle b,x\rangle + c)\) for some constants \(a,\) \(b,\)
and \(c.\)

Notice that

\[\frac{A_0}{A_1} = \frac{\exp(-1/2 B_0)}{\exp(-1/2 B_1)}
   = \exp\left(\frac{1}{2}(B_1-B_0)\right)
   = \exp\left(\frac{1}{2}\big(\langle 2\mu_0 - 2\mu_1, x\rangle+C\big)\right),\]

where \(C\) is based on the terms of \(B_j\) that use \(\mu_j\) but not
\(x.\) The scalar factor of \(A_0/A_1\) in (\ref{eq:gda_part2}) can also
be absorbed into the constant \(c\) in the expression
\(\exp(a\langle b, x\rangle + c).\) This confirms that the denominator
of (\ref{eq:gda_part2}) is indeed of the needed form, confirming the
goal (\ref{eq:gda_goal}).

\subsection{Naive Bayes}\label{naive-bayes}

I've heard that, in practice, naive Bayes works well for spam
classification. If its assumption is so blatantly false, then why does
it work so well? I don't have an answer, but I wanted to mention that as
an interesting question.

Also, independence of random variables is not just a pair-wise property.
You could have three random variables \(x_1,\) \(x_2,\) and \(x_3\)
which are all pair-wise independent, but not independent as a group. As
an example of this, here are three random variables defined as indicator
functions over the domain \(\{a, b, c, d\}:\)

\[\begin{aligned}
x_1(w) & = [w \in \{a, b\}] \\
x_2(w) & = [w \in \{b, d\}] \\
x_3(w) & = [w \in \{a, d\}] \\
\end{aligned}\]

If we know the value of \(x_i\) for any \(i,\) then we have no extra
information about the value of \(x_j\) for any \(j\ne i.\) However, if
we know any two values \(x_i\) and \(x_j,\) then we've completely
determined the value of \(x_k.\)

This is all leading up to the point that the naive Bayes assumption,
stated carefully, is that the entire set of events \(\{x_i \mid y\}\) is
independent, as opposed to just pair-wise independent.

I'm curious about any theoretical justification for Laplace smoothing.
As presented so far, it mainly seems like a hacky way to avoid a
potential implementation issue.

\section{On lecture notes 3}\label{on-lecture-notes-3}

The notes in this section are based on
\href{http://cs229.stanford.edu/notes/cs229-notes3.pdf}{lecture notes
3}.

\subsection{Support vector machines}\label{support-vector-machines}

At one point, Ng says that any constraint on \(w\) will work to avoid
the problem that maximizing the functional margin simply requires making
both \(w\) and \(b\) larger. However, I think many constraints make less
sense than \(||w||=1.\) The only case that doesn't fit well with
\(||w||=1\) is the case that the best weights are actually \(w=0\), but
this vector can only ever give a single output, so it's useless as long
as we have more than one label, which is the only interesting case. On
the other hand, a constraint like \(|w_3|=1\) would avoid the exploding
\((w,b)\) problem, but it may be the case that the best weight vector
has \(w_3=0,\) which would fail to be found under such a constraint. In
general, constraints tend to assume that some subset of values in
\((w,b)\) are nonzero, and anything more specific than \(||w||=1\) would
add an assumption that may end up missing the truly optimal weights.

\subsubsection{\texorpdfstring{max min \(\le\) min
max}{max min \textbackslash{}le min max}}\label{max-min-le-min-max}

Ng notes that for any function \(f(x, y),\)

\[\max_x \min_y f(x, y) \;\le\; \min_y \max_x f(x, y).\]

Let's justify this. Let

\newcommand{\argmax}{\text{argmax}}
\newcommand{\argmin}{\text{argmin}}

\[\begin{aligned}
x_0 & = \argmax_x \min_y f(x, y), \quad \text{and} \\
y_0 & = \argmin_y \max_x f(x, y).
\end{aligned}\]

Then

\[\begin{aligned}
\max_x \min_y f(x, y) & =   \; \min_y f(x_0, y) \\
                      & \le \; f(x_0, y_0)      \\
                      & \le \; \max_x f(x, y_0) = \min_y \max_x f(x, y), \\
\end{aligned}\]

which completes the proof.

There are only two inequalities in that proof; if they were equalities,
then the final result would itself be an equality as well. Specifically,
if \((x_0, y_0)\) is a saddle point in the sense that

\[f(x, y_0) \le f(x_0, y_0) \le f(x_0, y) \quad \forall x, y,\]

then

\[\max_x \min_y f(x, y) \; = \; \min_y \max_x f(x, y).\]

\subsubsection{The name of kernels}\label{the-name-of-kernels}

Quick side question: Why are kernels called kernels?

I found
\href{http://math.stackexchange.com/questions/500920/word-origin-meaning-of-kernel-in-linear-algebra}{this
link} which states that the word \emph{kernel}, in mathematics, was
first used in the context of a group homomorphism \(f\) to indicate the
set \(f^{-1}(0),\) the pre-image of the identity element. Interpreting
the word \emph{kernel} as meaning something like \emph{core} or
\emph{seed}, this usage in group theory makes sense when thinking about
\href{https://en.wikipedia.org/wiki/Fundamental_theorem_on_homomorphisms\#Group_theoretic_version}{the
first isomorphism theorem}.

In the context of SVMs, the name \emph{kernel} comes from the idea of a
\href{https://en.wikipedia.org/wiki/Positive-definite_kernel}{positive-definite
kernel}, which originated mainly in work done by James Mercer, although
as far as I can tell the actual name \emph{positive-definite kernel}
came from a 1904 paper of David Hilbert on Fredholm integral equations
of the second kind.

Based on \href{http://math.stackexchange.com/a/1281138/10785}{this
math.stackexchange answer}, it sounds like the group-theoretic and
algebraic use cases --- that is, the two listed above --- of the word
\emph{kernel} are not formally linked. Rather, it seems that Ivar
Fredholm used the French word \emph{noyau} in a paper on integral
equations that later became \emph{kernel} in the aforementioned paper of
Hilbert. \emph{Noyau} roughly means \emph{core}, and was perhaps used
because it is inside an integral when a convolution is performed. The
referenced answer states that the original Hilbert paper never justifies
the term, so I suppose it's up to folks who understand that paper to
make an educated guess as to why it was chosen; or by looking at the
corresponding Fredholm paper.

\subsubsection{The Gaussian kernel}\label{the-gaussian-kernel}

Let's confirm that the Gaussian kernel is indeed a kernel. As a side
note, this kernel is also referred to as the radial basis function
kernel, or the RBF kernel. I'll use \(K'\) for the function version and
\(K\) for the matrix version, both defined in a moment. The function is
defined as

\[K'(x, z) = \exp\left(-\frac{||x-z||^2}{2\sigma^2}\right).\]

Suppose we have a set of points \(\{x^{(1)}, \ldots, x^{(m)}\};\) define
matrix \(K\) by letting \(K_{ij} = K'(x^{(i)}, x^{(j)}).\) By Mercer's
theorem, our job is to show that, for any vector \(z,\) \(z^TKz \ge 0.\)

Let's proceed as follows:

\[\begin{aligned}
z^TKz & = z_i K_{ij} z_j \\
      & = z_i z_j \exp(-||x^{(i)} - x^{(j)}||^2/2\sigma^2)
\end{aligned}\]

\hypertarget{refs}{}

\end{document}

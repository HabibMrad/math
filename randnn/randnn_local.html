<!doctype html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="format-detection" content="telephone=no">
    <title>A Visual Exploration of Random Neural Networks</title>
	<script type="text/x-mathjax-config"> 
      MathJax.Hub.Register.StartupHook("TeX AMSmath Ready",function () { 
        MathJax.InputJax.TeX.Definitions.environment["densearray"] = 
          ['AMSarray',null,true,true,'rcl','0em .4em']; 
      }); 
	</script>
    <script src='/Users/tylerneylon/work/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
    <link rel="stylesheet" href="tufte-edited.css"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
  </head>
  <body>
    <p style="display:none">\(\newcommand{\latexonlyrule}[2]{}\)</p>
    <div class="main">
      <div class="title">A Visual Exploration of Random Neural Networks</div>
      <div class="author">Tyler Neylon — <span style="font-size: 80%">(Got a
          machine learning project? Email me:
          <a href="mailto:tyler@unboxresearch.com">tyler@unboxresearch.com</a>)
          </div></span>
      <div class="date">551.2018</div>
      <p>This article is an illustrated tour of neural networks in their primordial, untrained state. Neural networks are notoriously difficult beasts to understand. My aim is to provide a peek into the inherent beauty of this challenging world, and to build your intuition for how to set up neural networks through informed hyperparameter choices. I assume that you know neural networks are tools for machine learning, and that you have a little bit of a coding and math background, but I’ll try to keep things friendly.</p>
      <p>A random neural network looks like this:</p>
      <div class="figure">
      <p class="caption">Figure 1: The graph of a random feedforward neural network.</p>
      <img src="images/randnn2.png" alt="Figure 1: The graph of a random feedforward neural network." id="fig:fig1" />
      </div>
      <p>Why not be even more ambitious and throw in a few more dimensions? Here’s an animated cross-sectional view of another random network:</p>
      <div class="figure">
      <p class="caption">Figure 2: The 7-dimensional graph of a random feedforward neural network.</p>
        <iframe style="width:100%;height:244px"
            src="https://www.shadertoy.com/embed/XltBR4?paused=false&gui=false" frameborder="0"></iframe>
        <p style="width:100%;font-size:1.2em;text-align:center;margin-top:5px">
        <i>[If this animation isn't working for you, try the
            <a href="randnn_gif.html">animated gif instead</a>.]</i>
        </p>
      </div>



      <div class="figure">
      </div>

      <p>These are graphs of a specific type of model called a <em>feedforward neural network</em>, also known as a <em>multilayer perceptron</em>. In this article, I’ll explain how this kind of network is built out of simpler mathematical pieces. I’ll also visually show how many of the hyperparameter values for these models affect their behavior. Hyperparameters are values like the number of layers, layer size, and the probability distribution used to set the initial weights.</p>
      <p>Figure 1 above is based on a mathematical function that computes a color for every pixel. This function accepts two inputs, which are the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> coordinates for a pixel, and provides three outputs, which are the red, green, and blue intensities for that pixel. If I were to define such a function in Python, it would look like this:</p>
      <pre><code>def neural_net(x, y):
      
        # Compute r, g, and b using x and y.
        return r, g, b</code></pre>
      <p>A critical property of neural networks is that they are parametrizable through weight values you can adjust. Mathematically, we could choose to think of a neural network as a function <span class="math inline">\(f(x, w)\)</span> where both <span class="math inline">\(x\)</span> and <span class="math inline">\(w\)</span> are vectors. From this perspective, <span class="math inline">\(w\)</span> are the weights that are learned during training from a fixed set of data. Once training is complete, the vector <span class="math inline">\(w\)</span> remains fixed, while the function receives previously-unseen input values for <span class="math inline">\(x,\)</span> and we think of the <span class="math inline">\(x\)</span> values as the data from which we predict the desired output.</p>
      <p>Feedforward neural networks have a particular form. They’re built with weight matrices <span class="math inline">\(W_1, W_2, \ldots,\)</span> bias vectors <span class="math inline">\(b_1, b_2, \ldots,\)</span> and functions <span class="math inline">\(a_1(), a_2(), \ldots.\)</span> A three-layer feedforward network is built like so:</p>
      <p><span id="eq:eq0"><span class="math display">\[\begin{cases}
      y_0 = x \\
      y_1 = a_1(W_1 y_0 + b_1) \\
      y_2 = a_2(W_2 y_1 + b_2) \\
      y_3 = a_3(W_3 y_2 + b_3) \\
      f(x, w) = y_3 \\
      \end{cases}
      \qquad(1)\]</span></span></p>
      <p>The functions <span class="math inline">\(a_i()\)</span> are called <em>activation functions</em>. In practice, activation functions are very often chosen to be <span class="math inline">\(\tanh(x),\)</span> <span class="math inline">\(\max(0, x),\)</span> or variations on these.</p>
      <h1 id="a-single-layer"><span class="header-section-number">1</span> A Single Layer</h1>
      <p>Let’s take a look at a simple instance, and build our way up to the more sophisticated picture. Consider a single-layer model with matrix <span class="math inline">\(W_1\)</span> having weights given by <span class="math inline">\(w_{11} = 0.4\)</span> and <span class="math inline">\(w_{12} = 1.0,\)</span> bias term <span class="math inline">\(b_1=-0.3,\)</span> and the sigmoid activation function:</p>
      <p><span id="eq:eq1"><span class="math display">\[\sigma(x) = \frac{1}{1 + e^{-x}}.\qquad(2)\]</span></span></p>
      <p>Given inputs <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2,\)</span> we can summarize the entire neural network as a relatively simple function:</p>
      <p><span id="eq:eq2"><span class="math display">\[f(x) = a_1(W_1x + b_1) = \frac{1}{1+e^{-0.4x_1-x_2+0.3}}.\qquad(3)\]</span></span></p>
      <p>We can visualize this function as a 3D height plot, shown below in figure 3. Since the output values are constrained to the [0, 1] interval by the sigmoid function, we can also render the same plot as a pixel intensity image — basically a bird’s eye view in which values close to 1 appear white, values close to 0 appear black, and values between provide a gradient of grays.</p>
      <div class="figure">
      <p class="caption">Figure 3: On the left is the sigmoid function <span class="math inline">\(\sigma(x)\)</span> defined by (2). In the middle is a 3D heightmap of the single-layer, one-output neural network defined by (3). On the right is the overhead view of this same function.</p>
      <img src="images/gray_sigmoid@2x.png" alt="Figure 3: On the left is the sigmoid function \sigma(x) defined by (2). In the middle is a 3D heightmap of the single-layer, one-output neural network defined by (3). On the right is the overhead view of this same function." id="fig:fig3" />
      </div>
      <p>Here we see a single output value because our weight matrix <span class="math inline">\(W_1\)</span> had a single row. A layer can have multiple outputs, corresponding to multiple rows in its weight matrix, and a correspondingly sized bias vector (eg, <span class="math inline">\(W_1\)</span> and <span class="math inline">\(b_1\)</span>). Before the activation function is applied, each output can be seen as either a flat hyperplane (viewed as a heightmap) or as a linear color gradient (viewed as pixel intensities). After the activation, the output surface becomes nonlinear. If the activation function is <span class="math inline">\(\sigma(x)\)</span> or <span class="math inline">\(\tanh(x)\)</span>, then the outputs are mapped into a bounded interval; you might say that the hyperplanes have been flattened so that the ends become level, and there is a section of curvature in the middle.</p>
      <h1 id="the-second-layer"><span class="header-section-number">2</span> The Second Layer</h1>
      <p>Let’s dig a bit deeper into the rabbit hole and see what the next layer can do.</p>
      <p>We’ll start by building up to figure 4 below, which shows how a 2-layer network might behave when it has only a single (1-dimensional) input and a single (also 1-dimensional) output. Our example network uses the <span class="math inline">\(\tanh()\)</span> activation function. The output of the first layer is a series of similarly-shaped curves at different scales and locations (curves like this are often called <em>s-curves</em>); these are at left in the figure below. If we add together scaled version of these (in other words, if we take a <em>linear combination</em> of these), then we get a curve that, in a sense, has access to as many inflection points (intuitively, “turning points”) as it has inputs. An example is in the middle of the figure below. The final output of the second layer is the result of applying the activation function to this linear combination. The right-hand image in the figure shows <span class="math inline">\(\tanh()\)</span> applied to the curve shown in the middle of the figure.</p>
      <div class="figure">
      <p class="caption">Figure 4: A visual explanation of a simple 2-layer neural network. This network has a 1-dimensional input and a 1-dimensional output. Left: three different color-coded <span class="math inline">\(\tanh()\)</span> curves, each with its own weight and bias term. Middle: A linear combination of those three curves. Right: <span class="math inline">\(\tanh()\)</span> applied to the curve in the middle.</p>
      <img src="images/scurves@2x.png" alt="Figure 4: A visual explanation of a simple 2-layer neural network. This network has a 1-dimensional input and a 1-dimensional output. Left: three different color-coded \tanh() curves, each with its own weight and bias term. Middle: A linear combination of those three curves. Right: \tanh() applied to the curve in the middle." id="fig:fig4" />
      </div>
      <p>If we have more than a single input variable, then the outputs of the first layer look like those in figure 3 (for a <span class="math inline">\(\tanh()\)</span> or <span class="math inline">\(\sigma()\)</span> activation function), and the linear combinations of these can be more interesting since the inflection points turn into inflection <em>lines</em> (or <em>hyperplanes</em> in higher dimensions). The figure below is analogous to figure 4 just above; the change is that we now have two input variables instead of one.</p>
      <div class="figure">
      <p class="caption">Figure 5: Each of the three sigmoid-based surfaces on the left is a single output value of the first layer of a neural network. The middle surface is a linear combination of these three values. The right image is an overhead view of the middle plot.</p>
      <img src="images/3d_scurves@2x.png" alt="Figure 5: Each of the three sigmoid-based surfaces on the left is a single output value of the first layer of a neural network. The middle surface is a linear combination of these three values. The right image is an overhead view of the middle plot." id="fig:fig5" />
      </div>
      <p>If we have three different output values in the range [0, 1], then we can visualize all three at once by interpreting them as independent red, green, and blue channels, like so:</p>
      <div class="figure">
      <p class="caption">Figure 6: We can simultaneously render three different 2D plots by rending then as red, green, and blue intensities, respectively. This is an example using simple sigmoid curves.</p>
      <img src="images/rgb_added@2x.png" alt="Figure 6: We can simultaneously render three different 2D plots by rending then as red, green, and blue intensities, respectively. This is an example using simple sigmoid curves." id="fig:fig6" />
      </div>
      <p>The original figures in this article (figure 1 and figure 2) use this technique to render all three output values of a neural network in a single image.</p>
      <p>To wrap up, here are typical one-layer networks:</p>
      <div class="figure">
      <p class="caption">Figure 7: Several example one-layer networks.</p>
      <img src="images/typical1layers@2x.png" alt="Figure 7: Several example one-layer networks." id="fig:fig7" />
      </div>
      <p>Below are typical two-layer networks. Each of the following networks has 10 hidden units in its first layer.</p>
      <div class="figure">
      <p class="caption">Figure 8: Several example two-layer networks.</p>
      <img src="images/typical2layers@2x.png" alt="Figure 8: Several example two-layer networks." id="fig:fig8" />
      </div>
      <p>The weights in all of those have been randomly chosen, and I’ll mention the probability distribution used for them in a few moments.</p>
      <h1 id="the-deep-part-of-deep-learning"><span class="header-section-number">3</span> The Deep Part of Deep Learning</h1>
      <p>This section visualizes how networks change as we add more layers to them.</p>
      <p>The word <em>deep</em> in <em>deep learning</em> refers to the fact that many of today’s more sophisticated neural networks tend to have many layers. Building on our progression from 1- to 2-layer networks, what do networks with 3 or more layers look like?</p>
      <p>Here’s a progression:</p>
      <div class="figure">
      <p class="caption">Figure 9: Each column shows networks with the same number of layers. Images in the left column have 1 layer each; those in the right column have 10 layers. Within each row, the images show the output of successive layers of the <em>same</em> network. In other words, each image is a single layer added to the image to its left.</p>
      <img src="images/inc_depth@2x.png" alt="Figure 9: Each column shows networks with the same number of layers. Images in the left column have 1 layer each; those in the right column have 10 layers. Within each row, the images show the output of successive layers of the same network. In other words, each image is a single layer added to the image to its left." id="fig:fig9" />
      </div>
      <p>In the figure above, each row shows the output of successive layers of a feedforward neural network with random weights. The images in the first column are from the first layer; those in the second column are from the second layer; and so on. To phrase things intuitively, I’d say that deeper networks can capture a greater level of complexity in terms of behavior.</p>
      <p>Just for kicks, below is a deep dive on a single network. The final image consists of 18 layers. The hidden layers have 30 units each; in the language of definition (1), this means that each intermediate <span class="math inline">\(y_i\)</span> vector has 30 coordinates.</p>
      <div class="figure">
      <p class="caption">Figure 10: A focused look at one particular neural network. Read the images from left to right, then top to bottom. The top-left image shows outputs from the first layer alone. The bottom-right image shows outputs from the final, 18th layer.</p>
      <img src="images/adding_layers@2x.png" alt="Figure 10: A focused look at one particular neural network. Read the images from left to right, then top to bottom. The top-left image shows outputs from the first layer alone. The bottom-right image shows outputs from the final, 18th layer." id="fig:fig10" />
      </div>
      <p>Here’s a close-up of the above network extended to 27 layers:</p>
      <div class="figure">
      <p class="caption">Figure 11: The network from figure 10 with 27 layers.</p>
      <img src="images/fig11@2x.png" alt="Figure 11: The network from figure 10 with 27 layers." id="fig:fig11" />
      </div>
      <p>In terms of human-appreciable beauty, these graphs become fairly complex as we keep adding more layers. Here’s a glimpse of the same network extended to 40 layers:</p>
      <div class="figure">
      <p class="caption">Figure 12: The network from figure 10 extended to 40 layers.</p>
      <img src="images/fig12@2x.png" alt="Figure 12: The network from figure 10 extended to 40 layers." id="fig:fig12" />
      </div>
      <h1 id="activation-functions"><span class="header-section-number">4</span> Activation Functions</h1>
      <p>So far we’ve looked at two sigmoidal activation functions: <span class="math inline">\(\sigma()\)</span> and <span class="math inline">\(\tanh().\)</span> Another popular choice is <span class="math inline">\(\text{relu}(x) = \max(x, 0).\)</span></p>
      <div class="figure">
      <p class="caption">Figure 13: The function <span class="math inline">\(\text{relu}(x) = \max(x, 0).\)</span></p>
      <img src="images/relu@2x.png" alt="Figure 13: The function \text{relu}(x) = \max(x, 0)." id="fig:fig13" />
      </div>
      <p>In machine learning folklore, it is said that relu-based networks train more quickly than the sigmoidals. This may be true because, for positive <span class="math inline">\(x\)</span> values, <span class="math inline">\(\text{relu}(x)\)</span> has a derivative far from zero, whereas both <span class="math inline">\(\tanh()\)</span> and <span class="math inline">\(\sigma()\)</span> have small derivatives for large <span class="math inline">\(x\)</span> values. Functions with small derivatives result in slower convergence via gradient descent, making <span class="math inline">\(\text{relu}()\)</span> the better choice from this perspective.</p>
      <p>An interesting observation made by Ian Goodfellow (and perhaps by others before him) is that relu-based networks are <em>piecewise linear</em> functions, which means that we can partition the input space into distinct regions such that the network is a purely linear function in each region <span class="citation">(Goodfellow 2017)</span>. In the pixel-intensity visualization, this translates to an image that is entirely a patchwork of linear gradients.</p>
      <p>Let’s take a look at relu-based networks of varying depths. For comparison, the top row in the next figure is a <span class="math inline">\(\tanh()-\)</span>based network, similar to the networks above. The bottom row uses the <span class="math inline">\(\text{relu}()\)</span> activation function. Both networks use <span class="math inline">\(\sigma()\)</span> on their final layer in order to constrain the output values to the [0, 1] range.</p>
      <div class="figure">
      <p class="caption">Figure 14: Each row depicts a neural network at increasing depths. From left to right, each image has five more layers than the previous one. The top row uses tanh() activation functions; the bottom uses relu().</p>
      <img src="images/fig14@2x.png" alt="Figure 14: Each row depicts a neural network at increasing depths. From left to right, each image has five more layers than the previous one. The top row uses tanh() activation functions; the bottom uses relu()." id="fig:fig14" />
      </div>
      <h1 id="neurons-per-layer"><span class="header-section-number">5</span> Neurons Per Layer</h1>
      <p>In a layer like this:</p>
      <p><span class="math display">\[y_3 = \tanh(W_3 y_2 + b_3),\]</span></p>
      <p>the size (number of coordinates) of <span class="math inline">\(y_3\)</span> is referred to as either the number of <em>units</em> in the layer, the number of <em>neurons</em> in the layer, or simply the <em>size</em> of the layer. These are all synonyms.</p>
      <p>As the size of a layer increases, it’s capable of learning more features for the next layer to utilize. Both <em>layer size</em> and <em>network depth</em> (number of layers) are hyperparameters that can increase the complexity of the network. In figure 10, we saw a progression of a single network as its depth increased. Analogously, let’s take a look at a network as its layer size increases, from 10 neurons per layer up through 400 neurons per layer:</p>
      <div class="figure">
      <p class="caption">Figure 15: Random neural networks with increasing layer sizes. Each network has exactly 14 layers. From left to right, top to bottom, the networks have 10, 20, 40, 80, 120, and 400 neurons per layer, respectively.</p>
      <img src="images/inc_width@2x.png" alt="Figure 15: Random neural networks with increasing layer sizes. Each network has exactly 14 layers. From left to right, top to bottom, the networks have 10, 20, 40, 80, 120, and 400 neurons per layer, respectively." id="fig:fig15" />
      </div>
      <p>The final image above, with 400 neurons per layer, looks a bit like exploding static at this scale, which is the <span class="math inline">\(x, y\)</span> square <span class="math inline">\([-1, 1]^2.\)</span> Here’s a 30x zoom of this same network to show that it is indeed a continuous function, if a rather motley one:</p>
      <div class="figure">
      <p class="caption">Figure 16: A 30x zoom of the 400-neurons-per-layer network in the lower-right corner of figure 15.</p>
      <img src="images/400_neuron_zoom@2x.png" alt="Figure 16: A 30x zoom of the 400-neurons-per-layer network in the lower-right corner of figure 15." id="fig:fig16" />
      </div>
      <p>Comparing figure 10 (the effect of adding layers) to figure 15 (the effect of adding neurons per layer), I’d say that adding more layers to a network enables more complex <em>localized</em> behavior, whereas adding more neurons per layer enables more detailed <em>information throughout the entire model</em>. What I’m trying to capture is the difference between the uniformly splotchy texture of the 400-neurons-per-layer network in figure 15 versus the variety of both complex and relatively smooth regions that are present in the deep (40-layer) network in figure 12.</p>
      <h1 id="input-scale"><span class="header-section-number">6</span> Input Scale</h1>
      <p>It’s generally a good idea to scale your data so that it’s zero-centered and something close to normalized, such as having a standard deviation of 1. If we zoom into or out of a random neural network, we can get a sense for why this is useful. Let’s revisit my favorite random network from this article, the one from figure 11. Here is that same network as seen in the <span class="math inline">\([-2.6, 2.6]^2\)</span> square in the <span class="math inline">\(x, y-\)</span>plane:</p>
      <div class="figure">
      <p class="caption">Figure 17: For convenience, the same network we saw earlier in figure 11.</p>
      <img src="images/fig17@2x.png" alt="Figure 17: For convenience, the same network we saw earlier in figure 11." id="fig:fig17" />
      </div>
      <p>Next is a series of images starting with a wide view of the <span class="math inline">\([-40, 40]^2\)</span> square in the upper-left. Each subsequent image is at about twice the zoom level of the previous. The middle image is at about the same scale as the figure above.</p>
      <div class="figure">
      <p class="caption">Figure 18: The network from figure 17 shown at a number of different scales. The top-left image shows the square <span class="math inline">\([-40, 40]^2\)</span> in the <span class="math inline">\(x,y-\)</span>plane. Each subsequent image, read left to right, top to bottom, is at about twice the detail (so the second images shows the square <span class="math inline">\([-20, 20]^2\)</span>). The center of each image is the point <span class="math inline">\((0, 0).\)</span></p>
      <img src="images/fig18@2x.png" alt="Figure 18: The network from figure 17 shown at a number of different scales. The top-left image shows the square [-40, 40]^2 in the x,y-plane. Each subsequent image, read left to right, top to bottom, is at about twice the detail (so the second images shows the square [-20, 20]^2). The center of each image is the point (0, 0)." id="fig:fig18" />
      </div>
      <p>If input data to our network extended over a wide numerical range, or if it were far from zero-centered, then the top-left image above would be the most relevant, and we can see that the behavior of the network is essentially <em>radial</em> (meaning that it forms a star-like pattern where the distance from center is not an important parameter); in other words, the image appears to be constrained to a certain kind of behavior that may not fit the data.</p>
      <p>On the other hand, the bottom-right image is also less than ideal because it presents a relatively smooth, low-detail surface that’s likely to make training slower than it could be given stronger network gradients.</p>
      <p><em>(The next paragraph is slightly mathy — you can skip it if you’re not here for proofs.)</em></p>
      <p>There is a way to see why a zoomed-out random network has a radial pattern. As observed above, relu-based networks are truly piecewise linear, and in fact tanh()-based networks (like those in figure 18) are very close to piecewise linear in the sense that tanh() can be closely approximated with a very small number of linear pieces (I learned this from Ian Goodfellow <span class="citation">(Goodfellow 2017)</span>). From this perspective, the domain of each individual layer can be partitioned into <a href="https://en.wikipedia.org/wiki/Convex_set"><em>convex</em></a> regions on which the layer, as a function, is linear. The composition (that is, the application of layer after layer) of these functions maintains this property: the domain of multiple layers can still be partitioned into convex regions of linearity. Moreover, there are only ever finitely-many such regions. So the overall network domain, as a partition of linear-behavior regions, has finitely many convex pieces. Thus the pieces that are unbounded must necessarily form a star-like, or radial pattern similar to that of the top-left image in figure 18.</p>
      <h1 id="scale-of-random-weights"><span class="header-section-number">7</span> Scale of Random Weights</h1>
      <p>The last hyperparameter I’ll visualize for you is the scale of the random weights. There are different methods of choosing random weights, such as using a Gaussian distribution or a uniform distribution. For simplicity, all the networks in this article use uniformly random weights, although in practice normals are also great.</p>
      <p>The image grid below shows random networks with increasing random weight scales from left to right, top to bottom. All the other hyperparameters (such as number of layers and layer size) are held constant. Each network has 27 layers and 30 neurons each. The top-left network has random weights in the range [-0.5, 0.5]; the bottom-right network uses the range [-1, 1].</p>
      <div class="figure">
      <p class="caption">Figure 19: Each image shows essentially the same network, except that the scale of the random weights increases geometrically from the range [-0.5, 0.5] at the top left to [-1.0, 1.0] at the bottom right.</p>
      <img src="images/fig19@2x.png" alt="Figure 19: Each image shows essentially the same network, except that the scale of the random weights increases geometrically from the range [-0.5, 0.5] at the top left to [-1.0, 1.0] at the bottom right." id="fig:fig19" />
      </div>
      <p>Although the high-weight-scale images in the bottom row may <em>look</em> interesting, they’re not ideal for training because they are starting with an assumption that the data exhibits highly complex behavior. Any actual patterns in the data are likely to be far from this rather specific and arbitrary starting point.</p>
      <p>Below is an animation of the same network evolution shown in the image grid above. This animation provides a stronger sense of the infusion of detail as the weights scale up from the range <span class="math inline">\([-0.4, 0.4]\)</span> at the start of the animation to <span class="math inline">\([-0.8, 0.8]\)</span> at the end.</p>
      <div class="figure">
      <p class="caption">Figure 20: The network from figure 19 with weights gradually scaled up from the range [-0.4, 0.4] to the range [-0.8, 0.8].</p>
      <img src="images/weight_scale.gif" alt="Figure 20: The network from figure 19 with weights gradually scaled up from the range [-0.4, 0.4] to the range [-0.8, 0.8]." id="fig:fig20" />
      </div>
      <h2 id="some-math-behind-random-weights"><span class="header-section-number">7.1</span> Some Math Behind Random Weights</h2>
      <p>There are some useful techniques for choosing your initial random weights such as He or <a href="http://deepdish.io/2015/02/24/network-initialization/">Glorot</a> (aka Xavier) initialization that make it easier to find the sweet spot on this spectrum <span class="citation">(He et al. 2015)</span>, <span class="citation">(Glorot and Bengio 2010)</span>. These methods use expressions like <span class="math inline">\(1/\sqrt{n},\)</span> where <span class="math inline">\(n\)</span> is the number of inputs to a layer. I’ll provide a quick bit of intuition as to why that particular expression is a useful barometer for the scale of your random weights.</p>
      <p>Let’s build this intuition by looking at a single layer without a bias term or an activation function. These simplifications mean that we would have to do more follow-up work to achieve a fully justified approach, but this brief line of thinking will show you the kind of thought behind choosing a good initialization scale. Our simplified layer is a straightforward matrix multiplication:</p>
      <p><span class="math display">\[y = Wx.\]</span></p>
      <p>Suppose each term <span class="math inline">\(w_{ij}\)</span> in matrix <span class="math inline">\(W\)</span> is chosen independently with mean 0 and standard deviation <span class="math inline">\(\sigma.\)</span> Since the full network can have many layers, one of the key dangers is that the outputs of subsequent layers may either explode (each layer’s output is significantly larger than the previous output) or vanish (each layer’s output is significantly smaller than the previous). Mathematically, we can fight back by encouraging the output of a layer to have a similar scale to its input. In other words, we would like to encourage something like this:</p>
      <p><span id="eq:eq3"><span class="math display">\[\|y\| \approx \|x\|.\qquad(4)\]</span></span></p>
      <p>If you’re familiar with expected values and variances, then we can use those ideas to calculate the expected value of <span class="math inline">\(\|y\|^2\)</span> relative to the value of <span class="math inline">\(\|x\|^2:\)</span></p>
      <p><span class="math display">\[E\big[\|y\|^2\big] = \sum_i E\big[y_i^2\big] = \sum_i \text{Var}[y_i]\]</span></p>
      <p><span class="math display">\[ = \sum_{i,\,j} \text{Var}[w_{ij}x_j] = \sum_{i,\,j} x_j^2\text{Var}[w_{ij}]\]</span></p>
      <p><span class="math display">\[ = \sum_{i,\,j} x_j^2\sigma^2 = n\sigma^2\|x\|^2.\]</span></p>
      <p>In other words, <span class="math inline">\(E\big[\|y\|^2\big] = n\sigma^2 \|x\|^2.\)</span> Since we can choose the standard deviation of our weights <span class="math inline">\(\sigma,\)</span> we can set</p>
      <p><span class="math display">\[\sigma = \frac{1}{\sqrt{n}},\]</span></p>
      <p>and we’ll arrive at <span class="math inline">\(E\big[\|y\|^2\big] = \|x\|^2,\)</span> which nicely matches our original goal (4). This is not the exact reasoning used to justify He or Glorot initialization, but it does provide a first look at how the weight scales affect the global behavior of the network as a whole.</p>
      <hr />
      <p><em><strong>Hi!</strong> I hope you enjoyed my article. As you can see, I love machine learning. If you’d like to work together on a machine learning project, I’d love to hear from you. My company, Unbox Research, has a small team of talented ML engineers. We specialize in helping content platforms make more intelligent use of their data, which translates to algorithmic text and image comprehension as well as driving user engagement through discovery or personalization. I’m <a href="mailto:tyler@unboxresearch.com">tyler@unboxresearch.com</a>.</em></p>
      <p><em>(I’m also teaching a course on TensorFlow in New York in Jan 2019! <a href="http://unboxresearch.com/tf101">Register here.</a>)</em></p>
      <hr />
      <h1 id="notes-on-the-images"><span class="header-section-number">8</span> Notes on the Images</h1>
      <p>If you like the images in this article, I encourage you to render and play with them for yourself — they’re surprisingly delightful. As I experimented, I felt as if I was exploring a new world. This section explains how I created them.</p>
      <h2 id="figures-created-with-python"><span class="header-section-number">8.1</span> Figures Created with Python</h2>
      <p>All of the neural network images after figure 2, excluding the heightmaps, were created with <a href="https://github.com/tylerneylon/math/blob/master/randnn/notebook/randnn_images.ipynb">this Jupyter notebook</a>.</p>
      <h2 id="figures-created-with-grapher"><span class="header-section-number">8.2</span> Figures Created with Grapher</h2>
      <p>The line graphs and 3D heightmap graphs were generated with Grapher. The corresponding Grapher files can be found in <a href="https://github.com/tylerneylon/math/tree/master/randnn/grapher">this directory on GitHub</a>.</p>
      <h2 id="figures-created-with-shadertoy"><span class="header-section-number">8.3</span> Figures Created with Shadertoy</h2>
      <p>Figure 1 is a screenshot of a WebGL-renderable random neural network with the live-editable code <a href="https://www.shadertoy.com/view/MtdBz4">at this Shadertoy page</a>.</p>
      <p>There are two versions of figure 2 because I don’t think all browsers correctly display the live-rendered version, which uses WebGL. You can experiment with the live-rendered version in real time <a href="https://www.shadertoy.com/view/XltBR4">at this Shadertoy page</a>. The animated gif uses different parameters that you can find <a href="https://www.shadertoy.com/view/lldBz4">at this other Shadertoy page</a>.</p>
      <hr />
      <h1 id="references" class="unnumbered">References</h1>
      <div id="refs" class="references">
      <div id="ref-glorot2010understanding">
      <p>Glorot, Xavier, and Yoshua Bengio. 2010. “Understanding the Difficulty of Training Deep Feedforward Neural Networks.” In <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 249–56.</p>
      </div>
      <div id="ref-goodfellow2017cs231n">
      <p>Goodfellow, Ian. 2017. “Adversarial Examples and Adversarial Training (Stanford Cs231n, Lecture 16).” YouTube. <a href="https://www.youtube.com/watch?v=CIfsB_EYsVI#t=16m32s" class="uri">https://www.youtube.com/watch?v=CIfsB_EYsVI#t=16m32s</a>.</p>
      </div>
      <div id="ref-he2015delving">
      <p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on Imagenet Classification.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 1026–34.</p>
      </div>
      </div>
    </div>
  </body>
</html>
